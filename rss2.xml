<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"
  xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Skye</title>
    <link>http://yoursite.com/</link>
    <atom:link href="/rss2.xml" rel="self" type="application/rss+xml"/>
    
    <description></description>
    <pubDate>Thu, 29 Nov 2018 03:39:04 GMT</pubDate>
    <generator>http://hexo.io/</generator>
    
    <item>
      <title>Caffe &quot;in-place&quot; operation</title>
      <link>http://yoursite.com/2018/11/29/caffe_inplace/</link>
      <guid>http://yoursite.com/2018/11/29/caffe_inplace/</guid>
      <pubDate>Thu, 29 Nov 2018 02:48:25 GMT</pubDate>
      <description>
      
        
        
          &lt;p&gt;在学习Caffe model中关于Layers内容时，发现一个问题，ReLU层的top blob和bottom blob相同，如下：&lt;br&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;
        
      
      </description>
      
      <content:encoded><![CDATA[<p>在学习Caffe model中关于Layers内容时，发现一个问题，ReLU层的top blob和bottom blob相同，如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">layer &#123;</span><br><span class="line">  name: &quot;conv2/relu_3x3_reduce&quot;</span><br><span class="line">  type: &quot;ReLU&quot;</span><br><span class="line">  bottom: &quot;conv2/3x3_reduce&quot;</span><br><span class="line">  top: &quot;conv2/3x3_reduce&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>后来查资料发现这是caffe的in-place操作，是为了节省内（显）存，以及省去反复申请和释放内存的时间。</p><p>bottom blob和top blob名称相同，说明是同一个blob，占用的是内存的同一块空间。在这里也就是说，该ReLU操作是对输入blob本身进行操作，再将其输出。</p><p>如果定义的两个<code>layer</code>的top blob名称是一样的，那么这两个<code>layer</code>的bottom blob也一定是该top blob，并且按<code>layer</code>的定义顺序对该bottom blob进行操作。如果layer不是对bottom blob本身进行操作，那么top blob就不能与bottom blob相同，也不允许多个layer的top blob同名。因为假如这样做，后运算的layer会将top blob覆盖成其运算的结果，前面的layer的运算结果就消失了。</p><p>convolution、pooling层由于输入输出的size一般不一致，所以不能支持in-place操作；而ReLU函数是逐个对元素进行计算，不该变size，所以支持。AlexNet、VGGnet等都是在ReLU层使用in-place计算，ResNet中，BatchNorm和Scale也都使用了in-place计算。</p><p>目前已知支持in-place的有：ReLU层、Dropout层、BatchNorm层、Scale层。(除此之外，其他大部分layer貌似在定义时<code>name</code>和<code>top</code>是相同的。)</p>]]></content:encoded>
      
      <comments>http://yoursite.com/2018/11/29/caffe_inplace/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Yolov3 Network Architecture</title>
      <link>http://yoursite.com/2018/11/28/yolov3/</link>
      <guid>http://yoursite.com/2018/11/28/yolov3/</guid>
      <pubDate>Wed, 28 Nov 2018 09:59:59 GMT</pubDate>
      <description>
      
        
        
          &lt;p&gt;贴一张Yolov3网络结构图~&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://upload-images.jianshu.io/upload_images/14184962-7d6aca0fa7d27e52.jpg?imageMogr2/auto-orient/stri
        
      
      </description>
      
      <content:encoded><![CDATA[<p>贴一张Yolov3网络结构图~</p><p><img src="https://upload-images.jianshu.io/upload_images/14184962-7d6aca0fa7d27e52.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>filter = 3*(80+1+4) = 255</p><p>Yolov3 is a fully convolutional network. It has 75 convolutional layers, with skip connections and upsampling layers. No form of pooling is used, and a convolutional layer with stride 2 is used to downsample the feature maps, which helps in preventing loss of low-level features often attributed to pooling.</p><p>详细内容见：<a href="https://www.cyberailab.com/home/a-closer-look-at-yolov3" target="_blank" rel="noopener"><strong>A Closer Look at YOLOv3</strong></a></p>]]></content:encoded>
      
      <comments>http://yoursite.com/2018/11/28/yolov3/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Some notes for deep learning</title>
      <link>http://yoursite.com/2018/11/27/notes_for_dl/</link>
      <guid>http://yoursite.com/2018/11/27/notes_for_dl/</guid>
      <pubDate>Tue, 27 Nov 2018 15:41:55 GMT</pubDate>
      <description>
      
        
        
          &lt;h2 id=&quot;Object-localization&quot;&gt;&lt;a href=&quot;#Object-localization&quot; class=&quot;headerlink&quot; title=&quot;Object localization&quot;&gt;&lt;/a&gt;Object localization&lt;/h2&gt;&lt;p&gt;监督
        
      
      </description>
      
      <content:encoded><![CDATA[<h2 id="Object-localization"><a href="#Object-localization" class="headerlink" title="Object localization"></a>Object localization</h2><p>监督学习<br>训练样本中给出bounding box的中心点坐标和长宽，bx,by,bw,bh<br>Need to output bx,by,bw,bh,class label and Pc(is there any object?存在物体的置信度。当不存在物体的时候，loss函数只需计算Pc的准确度，其他项都不用考虑)。Y的维数为（1+4+class labels)。</p><hr><h2 id="Landmark-detection"><a href="#Landmark-detection" class="headerlink" title="Landmark detection"></a>Landmark detection</h2><p>特征点检测</p><p>需要输出一个是否有物体的置信度Pc,以及每个特征点的（x,y）坐标，所有训练集样本需包含这些labels。</p><p>面部表情识别，人体关键点检测。</p><hr><h2 id="Object-detection"><a href="#Object-detection" class="headerlink" title="Object detection"></a>Object detection</h2><h3 id="sliding-windows"><a href="#sliding-windows" class="headerlink" title="sliding windows"></a>sliding windows</h3><p>训练：根据滑动窗口的大小，使用适当裁剪的训练集样本，训练是否图片中有需要检测的物体物体。<br>检测：滑动窗口目标检测，对每一个窗口做一次检测，得到一个label,直到遍历图像每个区域。<br>缺点：computation expensive。步幅大时，粗粒度会影响检测性能，无法准确定位；细粒度或小步幅时计算成本高。所以一般使用简单的线性分类器。</p><h3 id="convolutional-implementation-of-sliding-windows"><a href="#convolutional-implementation-of-sliding-windows" class="headerlink" title="convolutional implementation of sliding windows"></a>convolutional implementation of sliding windows</h3><p>Turning FC layer into convolutional layers<br>没有全连接层时，输入图片的大小可以改变。<br>该卷积操作的原理是：不需要将需检测的输入图片分割成滑窗的大小的子集，分贝执行前向传播；而是直接将图片输入给卷积网络，其中的公共区域可以共享很多计算，一次性输出所有检测值。但边界框位置可能不够准确。</p><h3 id="bounding-box-prediction"><a href="#bounding-box-prediction" class="headerlink" title="bounding box prediction"></a>bounding box prediction</h3><p>YOLO algorithm</p><p>一个格子只存在一个物体时<br>用一个网格划分输入图片（3<em>3）<br>Labels for training for each grid cell:置信度Pc，bx,by,bw,bh, class labels<br>将物体分配给中心所在的格子(训练集中该格子的Pc=1)<br>output dimension:(1+4+class labels)\</em>3*3,<br>bx,by,bw,bh are specified relative to the grid cell(bbox中心所在cell的左上角坐标为0，右下角为1，bx,by是相对（0,0)的偏移量，值在0到1之间；bw,bh是bbox的长宽与grid cell边长的比值，可大于1。可以用sigmoid等函数处理，使其位于0到1之间）</p><h3 id="Intersection-over-Union"><a href="#Intersection-over-Union" class="headerlink" title="Intersection over Union"></a>Intersection over Union</h3><p>“Correct” if IoU&gt;=0.5(human chosen)<br>the predicted bbox with the ground truth(prior bbox)</p><h3 id="Non-max-Suppresion"><a href="#Non-max-Suppresion" class="headerlink" title="Non-max Suppresion"></a>Non-max Suppresion</h3><p>确保每个物体只被检测出一次。<br>先去掉所有Pc小于一定阈值的边界框(认为没有检测到物体），找出检测结果中Pc最大的bbox,然后去掉与其IoU大于阈值的检测框；再选择剩下的bbox中概率最高的…直到处理完所有框。<br>如果图像中有多中类别的物体，应该分别对每种label各<strong>独立</strong>做一次NMS。</p><hr><h3 id="Anchor-box"><a href="#Anchor-box" class="headerlink" title="Anchor box"></a>Anchor box</h3><p>使一个grid cell可以检测出多个物体。</p><p><strong>Previously</strong>:<br>each object in training image is assigned to grid cell that contains that object’s midpoint.</p><p><strong>With anchor boxes</strong>:<br>each object in training image is assigned to grid cell that contains object’s midpoint and anchor box for the grid cell with highest IoU. match with <strong>(grid cell, anchor box)</strong>.</p><p>for each grid cell, the output Y dimension is (1+4+class labels)*anchor boxes.</p><p>use k-means algorithm to choose anchor box.</p><p>grid cells越多，一个cell中出现多个物体的可能性更小。</p><hr><h2 id="Region-proposal-network-RPN"><a href="#Region-proposal-network-RPN" class="headerlink" title="Region proposal network(RPN)"></a>Region proposal network(RPN)</h2><p>two stages<br>利用图像分割算法，选出候选区域，在每种色块(blobs)上跑分类器。先找出可能的2000个色块，然后在这些色块上放置bbox，在这些色块上跑分类器。</p><p>R-CNN: Propose regions. Classify proposed regions one at a time. Output label + bounding box.</p><p>Fast R-CNN: Propose regions. Useconvolution implementation of sliding windows to classify all the proposed regions.</p><p>Faster R-CNN: Use convolutional network to propose regions.</p>]]></content:encoded>
      
      <comments>http://yoursite.com/2018/11/27/notes_for_dl/#disqus_thread</comments>
    </item>
    
    <item>
      <title>目标检测模型评估指标:mAP</title>
      <link>http://yoursite.com/2018/11/24/mAP/</link>
      <guid>http://yoursite.com/2018/11/24/mAP/</guid>
      <pubDate>Sat, 24 Nov 2018 02:19:28 GMT</pubDate>
      <description>
      
        
        
          &lt;p&gt;这些天看论文很多处用到mAP，一直只是知道这是一个评估指标，但具体怎么得来的不太清楚，现在有空学习一下~&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Mean Average Precision(mAP)用于评估目标检测模型的物体分类和定位性能，由于该类问题中每一个图片都可能包含许多不同类别的
        
      
      </description>
      
      <content:encoded><![CDATA[<p>这些天看论文很多处用到mAP，一直只是知道这是一个评估指标，但具体怎么得来的不太清楚，现在有空学习一下~</p><hr><p>Mean Average Precision(mAP)用于评估目标检测模型的物体分类和定位性能，由于该类问题中每一个图片都可能包含许多不同类别的物体，故图像分类问题的标准指标<em>precision</em>并不适用。</p><p><strong>Ground Truth</strong></p><p>对于目标检测问题，真实标签数据应包括图像中物体的类别以及该图像中每个物体的真实边界框。</p><p><strong>鉴别正确的检测结果并计算<em>precision</em>和<em>recall</em></strong></p><p>首先需得到<em>True Positives</em>、<em>False Positives</em>、<em>True Negatives</em>、<em>False Negatives</em>。</p><p>为了得到TP和FP，需要使用IoU，即预测框与<em>ground truth</em>的交集与并集之比，从而确定一个检测结果是正确的还是错误的。最常用的阈值为0.5，即IoU&gt;0.5，认为是TP，否则FP（当然在一些数据集的评估指标中常采用不同的阈值）。</p><p>计算<em>Recall</em>需要得到负样本的数量，但由于图片中并未预测到物体的每个部分都视作<em>Negative</em>，故很难得到TN。但可以只计算FN，即模型漏检的物体数。</p><p>另一个需要考虑的是模型所给出的各个检测结果的置信度。通过改变置信度阈值可以改变一个预测框是属于<em>Positive</em>还是<em>Negative</em>。阈值以上的所有预测（Box + Class）均被认为是<em>Positve</em>。</p><p>对于每一张图片，<em>ground truth</em>数据会给出图片中各个类别的实际物体数，计算每个<em>Positive</em>预测框与<em>ground truth</em>的IoU，并取值最大的预测框。然后根据IoU阈值，可以得到各个类别的TP和FP。由此，可计算出各个类别的<em>precision</em>。</p><p>由TP，可以计算得到漏检的物体数，即FN。由此，可以计算出各个类别的<em>recall</em>。</p><hr><p><strong>计算mAP</strong></p><p>mAP有许多不同的定义，在这里以PASCAL VOC竞赛的评估指标为例。</p><p>在这里，可以注意到，至少有两个变量会影响<em>precision</em>和<em>recall</em>，即IoU和置信度阈值。在PASCAL VOC竞赛中，IoU采用0.5。但置信度在不同模型中差异较大，会导致precision-recall曲线变化。于是，他们提出计算AP的一种方法。</p><p>首先对模型预测结果进行排序，按<strong>预测值置信度</strong>降序，给定一个<em>rank</em>，<em>recall</em>和<em>precision</em>仅在高于该<em>rank</em>值的预测结果中进行计算。改变<em>rank</em>会导致<em>recall</em>值的变化，共选择11个不同的<em>recall</em>值，分别为0,0.1,0.2,…,1.0，可认为选择了11个<em>rank</em>。由于按照置信度排序，实际就等价于选择了11个置信度阈值。</p><p><img src="https://upload-images.jianshu.io/upload_images/14184962-68a74391a33dcc79.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/600" alt=""></p><p>另外，在计算<em>precision</em>时，采用了插值的方法，对于某个<em>recall</em>值r，<em>precision</em>值取所有recall&gt;=r中的<strong>最大值</strong>，以保证PR曲线是单调递减的，减少由于样本排序中小的变化引起的PR曲线的抖动。AP就定义为在这11个<em>recall</em>下的<em>precision</em>的平均值，表征整个precision-recall曲线，也就是曲线下的面积。<em>average</em>是对<em>recall</em>取平均，<em>mean</em>是对所有类别取平均（每一个类别当做一次二分类任务）。</p><p>对于各个类别，用上述方法计算AP，求所有类别平均即得到mAP。<br>在COCO数据集中，采用的是更严格的计算方式，计算了不同IoU阈值和物体大小下的AP(详情见<a href="http://cocodataset.org/#detection-eval" target="_blank" rel="noopener">COCO Detection Evaluation</a>)。</p><p><strong>举个例子加深自己的理解：</strong></p><p>一个二分类问题，每个类分别五个样本，如果分类器性能足够好的话，那么根据预测的置信度降序排序，ranking结果应该是+1, +1, +1, +1, +1, -1, -1, -1, -1, -1。然而实际情况中，分类器预测的label和score都不可能如此完美；按score降序，加入根据给定的<em>rank</em>值，选择了前四个score(这个置信分数可能由softmax、SVM等方式计算得到)，认为这四个是正样本，由此来计算<em>recall</em>和<em>precision</em>。而实际上这四个样本中只有两个是正样本，那么此时的<em>recall</em>=2(选择中包含的正样本数)/5(总共的正样本数)=0.4，<em>precision</em>=2(选择中的正样本数)/4(认为是正样本的个数)=0.5。</p><p>可以看出，<em>recall</em>和<em>precision</em>均和<em>rank</em>值有关，也就是由此选择前k个样本，关于k的函数。那么根据<em>rank</em>取值的不同，这里总共可以计算10对precision-recall值，recall依次为为1/，将它们画出来，得到的就是PR曲线。</p><p>观察可以得到PR曲线的一个趋势就是，<em>recall</em>值越高，<em>precision</em>就越低。假如我选择所有样本来计算，那么当然包括了所有的正样本，此时的<em>recall</em>=1(此处认为ground truth所有框均被检测出来。这里有个问题需要思考下，在实际的目标检测问题中，是根据检测出的bbox来计算其与ground truth的IoU以判断bbox是否为正确检测，这样判定出的正确样本数为bbox的数量，往往不等于ground truth的数量；不过bbox的数量是需要经过NMS处理的，处理后基本与检测的目标数差不太多，)，<em>precision</em>就等于所有样本中正样本占比，当负样本占比很大时，那么<em>precision</em>就很小了。</p><hr><p>内容参考：<br>博客：<a href="http://tarangshah.com/blog/2018-01-27/what-is-map-understanding-the-statistic-of-choice-for-comparing-object-detection-models/" target="_blank" rel="noopener">Measuring Object Detection models - mAP - What is Mean Average Precision?</a><br>代码：<a href="https://github.com/facebookresearch/Detectron/blob/05d04d3a024f0991339de45872d02f2f50669b3d/lib/datasets/voc_eval.py" target="_blank" rel="noopener">VOC数据集的mAP实现</a>、<a href="https://github.com/cocodataset/cocoapi/blob/master/PythonAPI/pycocotools/cocoeval.py" target="_blank" rel="noopener">COCO数据集mAP计算API</a><br>论文：<a href="https://arxiv.org/pdf/1607.03476.pdf" target="_blank" rel="noopener">End-to-end training of object class detectors for mean average precision</a></p>]]></content:encoded>
      
      <comments>http://yoursite.com/2018/11/24/mAP/#disqus_thread</comments>
    </item>
    
    <item>
      <title>感受野（Receptive Field）</title>
      <link>http://yoursite.com/2018/11/23/ReceptiveField/</link>
      <guid>http://yoursite.com/2018/11/23/ReceptiveField/</guid>
      <pubDate>Fri, 23 Nov 2018 10:59:52 GMT</pubDate>
      <description>
      
        
        
          &lt;p&gt;之前一直不是很理解感受野是什么，因此认真地看了下相关知识，在这里记录一下~&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;感受野，指的是一个特定的CNN特征在输入空间所受影响的区域，可以用&lt;strong&gt;中心位置&lt;/strong&gt;(&lt;em&gt;center location&lt;/em&gt;)和&lt;stron
        
      
      </description>
      
      <content:encoded><![CDATA[<p>之前一直不是很理解感受野是什么，因此认真地看了下相关知识，在这里记录一下~</p><hr><p>感受野，指的是一个特定的CNN特征在输入空间所受影响的区域，可以用<strong>中心位置</strong>(<em>center location</em>)和<strong>大小</strong>(<em>size</em>)来表征；用来表示网络内部不同神经元对原图像的感受范围大小，也就是CNN每一层输出的特征图（<em>feature map</em>）上的像素点在原始图像上映射的区域大小。</p><p>由于网络结构中普遍使用卷积层和池化层，层与层之间均通过<em>sliding filter</em>进行局部相连，所以每一个神经元都无法对原始图像的所有信息进行感知。而感受野越大，表示该神经元所能接触到的原始图像范围越大，也就意味着更可能蕴含全局、语义层的深层特征；感受野越小则表示其所包含的特征更趋向于局部、浅层的细节特征。</p><p><strong>感受野的大小可以用来判断网络每一层的抽象层次。</strong></p><hr><p>输入层每个单元的感受野是1，层次越深，感受野越大，是由<em>kernel size</em>和<em>stride size</em>共同决定的。而对于深度CNN，我们无法直接追踪到感受野信息。</p><h2 id="感受野中心位置及大小计算"><a href="#感受野中心位置及大小计算" class="headerlink" title="感受野中心位置及大小计算"></a>感受野中心位置及大小计算</h2><p>具体公式推导及代码参考：<a href="https://medium.com/mlreview/a-guide-to-receptive-field-arithmetic-for-convolutional-neural-networks-e0f514068807" target="_blank" rel="noopener">A guide to receptive field arithmetic for Convolutional Neural Networks</a></p><hr><p>对于一个CNN特征来说，感受野中的每个像素值并不是同等重要。一个像素点越接近感受野中心，它对输出特征的计算所起作用越大，这意味着某一个特征不仅仅是受限在输入图片某个特定的区域，而且呈指数级聚焦在区域的中心。</p>]]></content:encoded>
      
      <comments>http://yoursite.com/2018/11/23/ReceptiveField/#disqus_thread</comments>
    </item>
    
    <item>
      <title>目标检测学习之路</title>
      <link>http://yoursite.com/2018/11/20/object_dection/</link>
      <guid>http://yoursite.com/2018/11/20/object_dection/</guid>
      <pubDate>Tue, 20 Nov 2018 03:41:41 GMT</pubDate>
      <description>
      
        
        
          &lt;p&gt;未写完…&lt;/p&gt;
&lt;p&gt;最近为了做毕设，重新开始看目标检测相关的一系列论文，在这里记录一下自己的学习过程，也方便以后复习。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&quot;R-CNN-2013-11&quot;&gt;&lt;a href=&quot;#R-CNN-2013-11&quot; class=&quot;headerlink&quot;
        
      
      </description>
      
      <content:encoded><![CDATA[<p>未写完…</p><p>最近为了做毕设，重新开始看目标检测相关的一系列论文，在这里记录一下自己的学习过程，也方便以后复习。</p><hr><h2 id="R-CNN-2013-11"><a href="#R-CNN-2013-11" class="headerlink" title="R-CNN / 2013.11"></a>R-CNN / 2013.11</h2><p>论文：<a href="https://arxiv.org/abs/1311.2524" target="_blank" rel="noopener">Rich feature hierarchies for accurate object detection and semantic segmentation</a></p><p><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Girshick%2C+R" target="_blank" rel="noopener">Ross Girshick</a>等人发表的这篇文章，首次将卷积神经网络用于「目标检测」，从这以后才有越来越多人将深度学习用于该问题，所以该论文意义深远。<br><img src="https://upload-images.jianshu.io/upload_images/14184962-b09b84954606caab.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/480" alt=""></p><p>主要解决了传统方法的两个问题：</p><p><strong>1. 速度</strong><br>R-CNN使用启发式方法（Selective Search) ，先生成候选区域再检测，降低信息的冗余度，从而提高了检测速度。</p><p><strong>2. 特征提取</strong><br>传统的手工提取，特征仅限于低层次的颜色、纹理等，故效果差。</p><hr><h2 id="SPP-Net-2014-6"><a href="#SPP-Net-2014-6" class="headerlink" title="SPP-Net / 2014.6"></a>SPP-Net / 2014.6</h2><p>论文：<a href="https://arxiv.org/abs/1406.4729" target="_blank" rel="noopener">Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition</a></p><p>在R-CNN提出半年后，<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=He%2C+K" target="_blank" rel="noopener">何恺明</a>等人提出了SPP-Net。</p><p>R-CNN中存在两个比较大的问题，SPP-Net就解决了这两个问题：<br><strong>1. 算力冗余</strong><br>先生成候选区域，再对区域进行卷积，这使得候选区域会有一定程度上的重叠，对相同的区域进行重复的卷积，而每个区域进行新的卷积还需要新的存储空间。</p><p>SPP-Net对此进行了优化，将先生成候选区域再卷积，改为先卷积再生成候选区域。这样，不仅减少了存储量而且加快了训练速度。</p><p><strong>2. 图片的裁剪与缩放</strong><br>在SPP-Net之前，所有的神经网络都是需要输入固定尺寸的照片，比如224*224（ImageNet）、32*32(LenNet)等。这样当我们希望检测各种大小的图片时，需要经过crop，或者warp等一系列操作，这都在一定程度上导致图片信息的丢失和变形，限制了识别精确度。而且，从生理学角度出发，人眼看到一个图片时，大脑会首先认为这是一个整体，而不会进行crop和warp，所以更有可能的是，我们的大脑通过搜集一些浅层的信息，在更深层才识别出这些任意形状的目标。<br><img src="https://upload-images.jianshu.io/upload_images/14184962-21a69ced5d0e9931.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/480" alt=""></p><p><strong>为什么需要固定输入图片的大小？</strong></p><p>卷积层的参数和输入大小无关，它仅是一个卷积核在图像上滑动，对不同大小的图片卷积得到不同大小的特征图（feature map)；但是<strong>全连接层（FC Layer）</strong>情况则不一样。由于全连接层把输入的所有像素点连接起来，需要指定输入层神经元个数和输出层神经元个数，所以必须规定输入的特征图大小，以指定参数个数。</p><p><strong>SPP-Net解决方法</strong></p><p> SPP-Net在最后一个卷积层后，接入了<strong>金字塔池化（spatial pyramid pooling)层</strong>，使用这种方式，可以让网络输入任意的图片，而且还会生成固定大小的输出。</p><p><strong>什么是金字塔池化？</strong><br><img src="https://upload-images.jianshu.io/upload_images/14184962-6b7b39eab6305a67.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/480" alt=""><br>从图中分析，黑色图片代表Conv5之后的feature map，其大小任意。接着以不同大小的块来提取特征，分别是4*4，2*2，1*1，将这三张网格放到下面这张特征图上，就可以得到16+4+1=21种不同的块(Spatial bins)，我们从这21个块中，每个块提取出一个特征，这样刚好就是我们要提取的21维特征向量。这种以不同的大小格子的组合方式来池化的过程就是空间金字塔池化（SPP）。如果要进行空间金字塔最大池化，就是从这21个图片块中，分别计算每个块的最大值，从而得到一个21维特征的输出单元。共用256个滤波器，最终输出向量大小固定为<strong>bins*filters</strong>，即（16+4+1)*256，作为全连接层的输入。</p><p>总的来说，金字塔池化意义在于<strong>多尺度特征提取出固定大小的特征向量</strong>。</p><hr><h2 id="Fast-R-CNN-2015-4"><a href="#Fast-R-CNN-2015-4" class="headerlink" title="Fast R-CNN / 2015.4"></a>Fast R-CNN / 2015.4</h2><p>论文：<a href="http://link.zhihu.com/?target=https%3A//arxiv.org/abs/1504.08083" target="_blank" rel="noopener">Fast R-CNN</a></p><p><a href="https://arxiv.org/abs/1504.08083" target="_blank" rel="noopener">Ross Girshick</a>在R-CNN之后，再次提出了Fast R-CNN，该论文中引用了SPP-Net的工作，相对于之前的R-CNN所做的最大优化就是<strong>快</strong>。</p><p><strong>将原来的串行结构改成并行结构</strong></p>]]></content:encoded>
      
      <comments>http://yoursite.com/2018/11/20/object_dection/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Hello World</title>
      <link>http://yoursite.com/2018/11/19/hello-world/</link>
      <guid>http://yoursite.com/2018/11/19/hello-world/</guid>
      <pubDate>Mon, 19 Nov 2018 07:50:57 GMT</pubDate>
      <description>
      
        
        
          &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
        
      
      </description>
      
      <content:encoded><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content:encoded>
      
      <comments>http://yoursite.com/2018/11/19/hello-world/#disqus_thread</comments>
    </item>
    
  </channel>
</rss>
