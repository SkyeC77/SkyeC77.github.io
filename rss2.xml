<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"
  xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Notes</title>
    <link>http://yoursite.com/</link>
    <atom:link href="/rss2.xml" rel="self" type="application/rss+xml"/>
    
    <description></description>
    <pubDate>Thu, 28 Feb 2019 08:56:20 GMT</pubDate>
    <generator>http://hexo.io/</generator>
    
    <item>
      <title>ERFNet eval</title>
      <link>http://yoursite.com/2019/02/28/README_eval/</link>
      <guid>http://yoursite.com/2019/02/28/README_eval/</guid>
      <pubDate>Thu, 28 Feb 2019 08:36:58 GMT</pubDate>
      <description>
      
        
        
          &lt;h1 id=&quot;Functions-for-evaluating-visualizing-the-network’s-output&quot;&gt;&lt;a href=&quot;#Functions-for-evaluating-visualizing-the-network’s-output&quot; clas
        
      
      </description>
      
      <content:encoded><![CDATA[<h1 id="Functions-for-evaluating-visualizing-the-network’s-output"><a href="#Functions-for-evaluating-visualizing-the-network’s-output" class="headerlink" title="Functions for evaluating/visualizing the network’s output"></a>Functions for evaluating/visualizing the network’s output</h1><p>Currently there are 4 usable functions to evaluate stuff:</p><ul><li>eval_cityscapes_color</li><li>eval_cityscapes_server</li><li>eval_iou</li><li>eval_forwardTime</li></ul><h2 id="eval-cityscapes-color-py"><a href="#eval-cityscapes-color-py" class="headerlink" title="eval_cityscapes_color.py"></a>eval_cityscapes_color.py</h2><p>This code can be used to produce segmentation of the Cityscapes images in color for visualization purposes. By default it saves images in eval/save_color/ folder. You can also visualize results in visdom with –visualize flag.</p><p><strong>Options:</strong> Specify the Cityscapes folder path with ‘–datadir’ option. Select the cityscapes subset with ‘–subset’ (‘val’, ‘test’, ‘train’ or ‘demoSequence’). For other options check the bottom side of the file.</p><p><strong>Examples:</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python eval_cityscapes_color.py --datadir /home/datasets/cityscapes/ --subset val</span><br></pre></td></tr></table></figure></p><h2 id="eval-cityscapes-server-py"><a href="#eval-cityscapes-server-py" class="headerlink" title="eval_cityscapes_server.py"></a>eval_cityscapes_server.py</h2><p>This code can be used to produce segmentation of the Cityscapes images and convert the output indices to the original ‘labelIds’ so it can be evaluated using the scripts from Cityscapes dataset (evalPixelLevelSemanticLabeling.py) or uploaded to Cityscapes test server. By default it saves images in eval/save_results/ folder.</p><p><strong>Options:</strong> Specify the Cityscapes folder path with ‘–datadir’ option. Select the cityscapes subset with ‘–subset’ (‘val’, ‘test’, ‘train’ or ‘demoSequence’). For other options check the bottom side of the file.</p><p><strong>Examples:</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python eval_cityscapes_server.py --datadir /home/datasets/cityscapes/ --subset val</span><br></pre></td></tr></table></figure></p><h2 id="eval-iou-py"><a href="#eval-iou-py" class="headerlink" title="eval_iou.py"></a>eval_iou.py</h2><p>This code can be used to calculate the IoU (mean and per-class) in a subset of images with labels available, like Cityscapes val/train sets.</p><p><strong>Options:</strong> Specify the Cityscapes folder path with ‘–datadir’ option. Select the cityscapes subset with ‘–subset’ (‘val’ or ‘train’). For other options check the bottom side of the file.</p><p><strong>Examples:</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python eval_iou.py --datadir /home/datasets/cityscapes/ --subset val</span><br></pre></td></tr></table></figure></p><h2 id="eval-forwardTime-py"><a href="#eval-forwardTime-py" class="headerlink" title="eval_forwardTime.py"></a>eval_forwardTime.py</h2><p>This function loads a model specified by ‘-m’ and enters a loop to continuously estimate forward pass time (fwt) in the specified resolution. </p><p><strong>Options:</strong> Option ‘–width’ specifies the width (default: 1024). Option ‘–height’ specifies the height (default: 512). For other options check the bottom side of the file.</p><p><strong>Examples:</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python eval_forwardTime.py</span><br></pre></td></tr></table></figure></p><p><strong>NOTE</strong>: Paper values were obtained with a single Titan X (Maxwell) and a Jetson TX1 using the original Torch code. The pytorch code is a bit faster, but cudahalf (FP16) seems to give problems at the moment for some pytorch versions so this code only runs at FP32 (a bit slower).</p>]]></content:encoded>
      
      <comments>http://yoursite.com/2019/02/28/README_eval/#disqus_thread</comments>
    </item>
    
    <item>
      <title>ERFNet train</title>
      <link>http://yoursite.com/2019/02/28/README_train/</link>
      <guid>http://yoursite.com/2019/02/28/README_train/</guid>
      <pubDate>Thu, 28 Feb 2019 08:36:27 GMT</pubDate>
      <description>
      
        
        
          &lt;h1 id=&quot;Training-ERFNet-in-Pytorch&quot;&gt;&lt;a href=&quot;#Training-ERFNet-in-Pytorch&quot; class=&quot;headerlink&quot; title=&quot;Training ERFNet in Pytorch&quot;&gt;&lt;/a&gt;Training
        
      
      </description>
      
      <content:encoded><![CDATA[<h1 id="Training-ERFNet-in-Pytorch"><a href="#Training-ERFNet-in-Pytorch" class="headerlink" title="Training ERFNet in Pytorch"></a>Training ERFNet in Pytorch</h1><p>PyTorch code for training ERFNet model on Cityscapes. The code was based initially on the code from <a href="https://github.com/bodokaiser/piwise" target="_blank" rel="noopener">bodokaiser/piwise</a>, adapted with several custom added modifications and tweaks. Some of them are:</p><ul><li>Load cityscapes dataset</li><li>ERFNet model definition</li><li>Calculate IoU on each epoch during training</li><li>Save snapshots and best model during training</li><li>Save additional output files useful for checking results (see below “Output files…”)</li><li>Resume training from checkpoint (use “–resume” flag in the command)</li></ul><h2 id="Options"><a href="#Options" class="headerlink" title="Options"></a>Options</h2><p>For all options and defaults please see the bottom of the “main.py” file. Required ones are –savedir (name for creating a new folder with all the outputs of the training) and –datadir (path to cityscapes directory).</p><h2 id="Example-commands"><a href="#Example-commands" class="headerlink" title="Example commands"></a>Example commands</h2><p>Train encoder with 150 epochs and batch=6 and then train decoder (decoder training starts after encoder training):<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python main.py --savedir erfnet_training1 --datadir /home/datasets/cityscapes/ --num-epochs 150 --batch-size 6</span><br></pre></td></tr></table></figure></p><p>Train decoder using encoder’s pretrained weights with ImageNet:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python main.py --savedir erfnet_training1 --datadir /home/datasets/cityscapes/ --num-epochs 150 --batch-size 6 --decoder --pretrainedEncoder &quot;../trained_models/erfnet_encoder_pretrained.pth.tar&quot;</span><br></pre></td></tr></table></figure></p><h2 id="Output-files-generated-for-each-training"><a href="#Output-files-generated-for-each-training" class="headerlink" title="Output files generated for each training:"></a>Output files generated for each training:</h2><p>Each training will create a new folder in the “erfnet_pytorch/save/“ directory named with the parameter –savedir and the following files:</p><ul><li><strong>automated_log.txt</strong>: Plain text file that contains in columns the following info of each epoch {Epoch, Train-loss,Test-loss,Train-IoU,Test-IoU, learningRate}. Can be used to plot using Gnuplot or Excel.</li><li><strong>best.txt</strong>: Plain text file containing a line with the best IoU achieved during training and its epoch.</li><li><strong>checkpoint.pth.tar</strong>: bundle file that contains the checkpoint of the last trained epoch, contains the following elements: ‘epoch’ (epoch number as int), ‘arch’ (net definition as a string), ‘state_dict’ (saved weights dictionary loadable by pytorch), ‘best_acc’ (best achieved accuracy as float), ‘optimizer’ (saved optimizer parameters).</li><li><strong>{model}.py</strong>: copy of the model file used (default erfnet.py). </li><li><strong>model.txt</strong>: Plain text that displays the model’s layers</li><li><strong>model_best.pth</strong>: saved weights of the epoch that achieved best val accuracy.</li><li><strong>model_best.pth.tar</strong>: Same parameters as “checkpoint.pth.tar” but for the epoch with best val accuracy.</li><li><strong>opts.txt</strong>: Plain text file containing the options used for this training</li></ul><p>NOTE: Encoder trainings have an added “_encoder” tag to each file’s name.</p><h2 id="IoU-display-during-training"><a href="#IoU-display-during-training" class="headerlink" title="IoU display during training"></a>IoU display during training</h2><p>NEW: In previous code, IoU was calculated using a port of the cityscapes scripts, but new code has been added in “iouEval.py” to make it class-general, non-dependable on other code, and much faster (using cuda)</p><p>By default, only Validation IoU is calculated for faster training (can be changed in options)</p><h2 id="Visualization"><a href="#Visualization" class="headerlink" title="Visualization"></a>Visualization</h2><p>If you want to visualize the outputs during training add the “–visualize” flag and open an extra tab with:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m visdom.server -port 8097</span><br></pre></td></tr></table></figure></p><p>The plots will be available using the browser in <a href="http://localhost.com:8097" target="_blank" rel="noopener">http://localhost.com:8097</a></p><h2 id="Multi-GPU"><a href="#Multi-GPU" class="headerlink" title="Multi-GPU"></a>Multi-GPU</h2><p>If you wish to specify which GPUs to use, use the CUDA_VISIBLE_DEVICES command:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=0 python main.py ...</span><br><span class="line">CUDA_VISIBLE_DEVICES=0,1 python main.py ...</span><br></pre></td></tr></table></figure></p>]]></content:encoded>
      
      <comments>http://yoursite.com/2019/02/28/README_train/#disqus_thread</comments>
    </item>
    
    <item>
      <title>ERFNet overview</title>
      <link>http://yoursite.com/2019/01/18/README1/</link>
      <guid>http://yoursite.com/2019/01/18/README1/</guid>
      <pubDate>Fri, 18 Jan 2019 13:29:22 GMT</pubDate>
      <description>
      
        
        
          &lt;h1 id=&quot;ERFNet-PyTorch-version&quot;&gt;&lt;a href=&quot;#ERFNet-PyTorch-version&quot; class=&quot;headerlink&quot; title=&quot;ERFNet (PyTorch version)&quot;&gt;&lt;/a&gt;ERFNet (PyTorch ve
        
      
      </description>
      
      <content:encoded><![CDATA[<h1 id="ERFNet-PyTorch-version"><a href="#ERFNet-PyTorch-version" class="headerlink" title="ERFNet (PyTorch version)"></a>ERFNet (PyTorch version)</h1><p>This code is a toolbox that uses <strong>PyTorch</strong> for training and evaluating the <strong>ERFNet</strong> architecture for semantic segmentation.</p><p><strong>For the Original Torch version please go <a href="https://github.com/Eromera/erfnet" target="_blank" rel="noopener">HERE</a></strong></p><p>NOTE: This PyTorch version has a slightly better result than the ones in the Torch version (used in the paper): 72.1 IoU in Val set and 69.8 IoU in test set.</p><p><img src="/2019/01/18/README1/example_segmentation.png?raw=true" alt="Example segmentation" title="Example segmentation"></p><h2 id="Publications"><a href="#Publications" class="headerlink" title="Publications"></a>Publications</h2><p>If you use this software in your research, please cite our publications:</p><p><strong>“Efficient ConvNet for Real-time Semantic Segmentation”</strong>, E. Romera, J. M. Alvarez, L. M. Bergasa and R. Arroyo, IEEE Intelligent Vehicles Symposium (IV), pp. 1789-1794, Redondo Beach (California, USA), June 2017.<br><strong>[Best Student Paper Award]</strong>, <a href="http://www.robesafe.uah.es/personal/eduardo.romera/pdfs/Romera17iv.pdf" target="_blank" rel="noopener">[pdf]</a></p><p><strong>“ERFNet: Efficient Residual Factorized ConvNet for Real-time Semantic Segmentation”</strong>, E. Romera, J. M. Alvarez, L. M. Bergasa and R. Arroyo, Transactions on Intelligent Transportation Systems (T-ITS), December 2017. <a href="http://www.robesafe.uah.es/personal/eduardo.romera/pdfs/Romera17tits.pdf" target="_blank" rel="noopener">[pdf]</a></p><h2 id="Packages"><a href="#Packages" class="headerlink" title="Packages"></a>Packages</h2><p>For instructions please refer to the README on each folder:</p><ul><li><a href="train">train</a> contains tools for training the network for semantic segmentation.</li><li><a href="eval">eval</a> contains tools for evaluating/visualizing the network’s output.</li><li><a href="imagenet">imagenet</a> Contains script and model for pretraining ERFNet’s encoder in Imagenet.</li><li><a href="trained_models">trained_models</a> Contains the trained models used in the papers. NOTE: the pytorch version is slightly different from the torch models.</li></ul><h2 id="Requirements"><a href="#Requirements" class="headerlink" title="Requirements:"></a>Requirements:</h2><ul><li><a href="https://www.cityscapes-dataset.com/" target="_blank" rel="noopener"><strong>The Cityscapes dataset</strong></a>: Download the “leftImg8bit” for the RGB images and the “gtFine” for the labels. <strong>Please note that for training you should use the “_labelTrainIds” and not the “_labelIds”, you can download the <a href="https://github.com/mcordts/cityscapesScripts" target="_blank" rel="noopener">cityscapes scripts</a> and use the <a href="https://github.com/mcordts/cityscapesScripts/blob/master/cityscapesscripts/preparation/createTrainIdLabelImgs.py" target="_blank" rel="noopener">conversor</a> to generate trainIds from labelIds</strong></li><li><a href="https://www.python.org/" target="_blank" rel="noopener"><strong>Python 3.6</strong></a>: If you don’t have Python3.6 in your system, I recommend installing it with <a href="https://www.anaconda.com/download/#linux" target="_blank" rel="noopener">Anaconda</a></li><li><a href="http://pytorch.org/" target="_blank" rel="noopener"><strong>PyTorch</strong></a>: Make sure to install the Pytorch version for Python 3.6 with CUDA support (code only tested for CUDA 8.0). </li><li><strong>Additional Python packages</strong>: numpy, matplotlib, Pillow, torchvision and visdom (optional for –visualize flag)</li></ul><p>In Anaconda you can install with:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda install numpy matplotlib torchvision Pillow</span><br><span class="line">conda install -c conda-forge visdom</span><br></pre></td></tr></table></figure></p><p>If you use Pip (make sure to have it configured for Python3.6) you can install with: </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install numpy matplotlib torchvision Pillow visdom</span><br></pre></td></tr></table></figure><h2 id="License"><a href="#License" class="headerlink" title="License"></a>License</h2><p>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License, which allows for personal and research use only. For a commercial license please contact the authors. You can view a license summary here: <a href="http://creativecommons.org/licenses/by-nc/4.0/" target="_blank" rel="noopener">http://creativecommons.org/licenses/by-nc/4.0/</a></p>]]></content:encoded>
      
      <comments>http://yoursite.com/2019/01/18/README1/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Caffe--&quot;deploy.prototxt&quot; and &quot;train_val.prototxt&quot;</title>
      <link>http://yoursite.com/2018/12/15/deploy/</link>
      <guid>http://yoursite.com/2018/12/15/deploy/</guid>
      <pubDate>Sat, 15 Dec 2018 01:55:12 GMT</pubDate>
      <description>
      
        
        
          &lt;p&gt;最近学习了一下Caffe，并且用迁移学习完成了一个简单的猫狗分类问题。在学习的过程中，踩了很多坑，这里主要记录一下关于几个prototxt配置文件的问题。&lt;/p&gt;
&lt;h2 id=&quot;prototxt文件&quot;&gt;&lt;a href=&quot;#prototxt文件&quot; class=&quot;header
        
      
      </description>
      
      <content:encoded><![CDATA[<p>最近学习了一下Caffe，并且用迁移学习完成了一个简单的猫狗分类问题。在学习的过程中，踩了很多坑，这里主要记录一下关于几个prototxt配置文件的问题。</p><h2 id="prototxt文件"><a href="#prototxt文件" class="headerlink" title="prototxt文件"></a>prototxt文件</h2><ul><li>train_val.prototxt：网络配置文件，用于训练</li><li>deploy.prototxt：网络配置文件，用于inference，测试</li><li>solver.prototxt：参数文件，用于训练，设置了学习率、优化方法等，slover函数生成</li></ul><h2 id="fine-tune"><a href="#fine-tune" class="headerlink" title="fine-tune"></a>fine-tune</h2><p>为了将在ImageNet上训练好的模型用于自己的数据集分类问题，我们需要对网络进行迁移学习或者微调。</p><p>下面以GoogleNet为例对几点需要注意的地方进行说明：</p><ol><li>先准备好训练数据与测试数据。在train_val.prototxt文件中，如果输入的数据可以是图片格式，也可以是存储在数据库中的lmdb格式，可以加快读取速度。两种情况下数据层的配置有区别，如下：</li></ol><p><strong>Image格式</strong>：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">layer &#123;</span><br><span class="line">  name: &quot;data&quot;</span><br><span class="line">  type: &quot;ImageData&quot;</span><br><span class="line">  top: &quot;data&quot;</span><br><span class="line">  top: &quot;label&quot;</span><br><span class="line">  include &#123;</span><br><span class="line">    phase: TRAIN</span><br><span class="line">  &#125;</span><br><span class="line">  image_data_param &#123;</span><br><span class="line">    source: &quot;train.txt&quot;</span><br><span class="line">    new_height: 224</span><br><span class="line">    new_width: 224</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">layer &#123;</span><br><span class="line">  name: &quot;data&quot;</span><br><span class="line">  type: &quot;ImageData&quot;</span><br><span class="line">  top: &quot;data&quot;</span><br><span class="line">  top: &quot;label&quot;</span><br><span class="line">  include &#123;</span><br><span class="line">    phase: TEST</span><br><span class="line">  &#125;</span><br><span class="line">  image_data_param &#123;</span><br><span class="line">    source: &quot;val.txt&quot;</span><br><span class="line">    batch_size: 50</span><br><span class="line">    new_height: 224</span><br><span class="line">    new_width: 224</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>其中train.txt和val.txt分别是保存了训练集和测试集的路径的文件，直接以图片格式读入。</p><p><strong>lmdb格式</strong>：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">layer &#123;</span><br><span class="line">  name: &quot;data&quot;</span><br><span class="line">  type: &quot;Data&quot;</span><br><span class="line">  top: &quot;data&quot;</span><br><span class="line">  top: &quot;label&quot;</span><br><span class="line">  include &#123;</span><br><span class="line">    phase: TRAIN</span><br><span class="line">  &#125;</span><br><span class="line">  transform_param &#123;</span><br><span class="line">    mirror: true</span><br><span class="line">    crop_size: 224</span><br><span class="line">    mean_value: 104</span><br><span class="line">    mean_value: 117</span><br><span class="line">    mean_value: 123</span><br><span class="line">  &#125;</span><br><span class="line">  data_param &#123;</span><br><span class="line">    source: &quot;examples/imagenet/ilsvrc12_train_lmdb&quot;</span><br><span class="line">    batch_size: 32</span><br><span class="line">    backend: LMDB</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">layer &#123;</span><br><span class="line">  name: &quot;data&quot;</span><br><span class="line">  type: &quot;Data&quot;</span><br><span class="line">  top: &quot;data&quot;</span><br><span class="line">  top: &quot;label&quot;</span><br><span class="line">  include &#123;</span><br><span class="line">    phase: TEST</span><br><span class="line">  &#125;</span><br><span class="line">  transform_param &#123;</span><br><span class="line">    mirror: false</span><br><span class="line">    crop_size: 224</span><br><span class="line">    mean_value: 104</span><br><span class="line">    mean_value: 117</span><br><span class="line">    mean_value: 123</span><br><span class="line">  &#125;</span><br><span class="line">  data_param &#123;</span><br><span class="line">    source: &quot;examples/imagenet/ilsvrc12_val_lmdb&quot;</span><br><span class="line">    batch_size: 50</span><br><span class="line">    backend: LMDB</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>这个需要自己利用train.txt和val.txt文件从硬盘读取图像数据保存在LMDB数据库，该数据格式采用内存-映射，故I/O效率高。</p><ol start="2"><li><p>需要改变最后一个fc层的名称（type:”InnerProduct”），并将输出维度改为我们的类别数，也就相当于重头开始学习一个新的层，这样在训练时，模型就会去fit我们的训练数据。</p></li><li><p>当自己的数据集够大时，可以在训练过程中对所有网络权重进行更新；但是本身任务的数据集较小时，为了避免过拟合，需要冻结一部分网络，一般是选择固定前面层的参数，不更新权重，只更新最后几层（迁移学习时冻结前面所有层，只对最后一个fc层进行权重更新，训练分类器）。这是因为网络的前几层得到的特征是比较基础local的特征，基本适用于全部任务（边缘，角点特征），而网络层数越高的层，和全局信息联系越紧密，local的信息是多数情况共享的，而global的信息和原图紧密相关。</p></li></ol><p>具体的不更新权重的方法在caffe中就是将该层param中的学习率设为0：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">layer &#123;</span><br><span class="line">  name: &quot;conv1/7x7_s2&quot;</span><br><span class="line">  type: &quot;Convolution&quot;</span><br><span class="line">  bottom: &quot;data&quot;</span><br><span class="line">  top: &quot;conv1/7x7_s2&quot;</span><br><span class="line">  param &#123;</span><br><span class="line">    lr_mult: 0</span><br><span class="line">    decay_mult: 1</span><br><span class="line">  &#125;</span><br><span class="line">  param &#123;</span><br><span class="line">    lr_mult: 0</span><br><span class="line">    decay_mult: 0</span><br><span class="line">  &#125;</span><br><span class="line">  convolution_param &#123;</span><br><span class="line">    num_output: 64</span><br><span class="line">    pad: 3</span><br><span class="line">    kernel_size: 7</span><br><span class="line">    stride: 2</span><br><span class="line">    weight_filler &#123;</span><br><span class="line">      type: &quot;xavier&quot;</span><br><span class="line">    &#125;</span><br><span class="line">    bias_filler &#123;</span><br><span class="line">      type: &quot;constant&quot;</span><br><span class="line">      value: 0.2</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><ol start="4"><li><p>在训练过程中一般将偏置项b的学习率设置为权重矩阵w的两倍，最终的学习率等于train_val.prototxt中的lr_mult*base_lr（slover.prototxt中设置）。在fine-tune时，base_lr要设置得很小，一般在1e-5数量级，而最后一层的lr_mult设置得较大，因为该层是重新学习，一般是其它层的10倍。</p></li><li><p>solver参数都需要进行修改，原来的参数适用于training from scratch，学习率、步长、迭代次数都较大，我们都需要改得比较小。</p></li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">#Based on https://github.com/BVLC/caffe/tree/master/models/bvlc_googlenet</span><br><span class="line">net: &quot;train_val.prototxt&quot;</span><br><span class="line">display: 50</span><br><span class="line">base_lr: 0.0001</span><br><span class="line">lr_policy: &quot;fixed&quot;</span><br><span class="line">max_iter: 15000</span><br><span class="line">momentum: 0.9</span><br><span class="line">weight_decay: 0.0002</span><br><span class="line">snapshot_prefix: &quot;transfer_learning&quot;</span><br><span class="line">solver_mode: CPU</span><br><span class="line">test_iter: 100</span><br><span class="line">test_interval: 1000</span><br></pre></td></tr></table></figure><ol start="6"><li>在做inference时，使用deploy.prototxt文件，其实是train_val.prototxt文件改动了前面的数据层以及最后的计算loss的部分。这时数据层很简单：</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">layer &#123;</span><br><span class="line">  name: &quot;data&quot;</span><br><span class="line">  type: &quot;Input&quot;</span><br><span class="line">  top: &quot;data&quot;</span><br><span class="line">  input_param &#123; shape: &#123; dim: 1 dim: 3 dim: 224 dim: 224 &#125; &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>而针对内积之后的softmax输出层，训练和测试时也有差别：</p><p>training:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">layer &#123;</span><br><span class="line">  name: &quot;loss3/loss3&quot;</span><br><span class="line">  type: &quot;SoftmaxWithLoss&quot;</span><br><span class="line">  bottom: &quot;loss3/classifier_dc&quot;</span><br><span class="line">  bottom: &quot;label&quot;</span><br><span class="line">  top: &quot;loss3/loss3&quot;</span><br><span class="line">  loss_weight: 1</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>inference:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">layer &#123;</span><br><span class="line">  name: &quot;prob&quot;</span><br><span class="line">  type: &quot;Softmax&quot;</span><br><span class="line">  bottom: &quot;loss3/classifier_dc&quot;</span><br><span class="line">  top: &quot;prob&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>因为测试时不需要计算误差进行bp，也就不需要label，故bottom层只有一个，type也不同。</p>]]></content:encoded>
      
      <comments>http://yoursite.com/2018/12/15/deploy/#disqus_thread</comments>
    </item>
    
    <item>
      <title>正则化与先验分布的关系</title>
      <link>http://yoursite.com/2018/12/14/regularization/</link>
      <guid>http://yoursite.com/2018/12/14/regularization/</guid>
      <pubDate>Fri, 14 Dec 2018 01:20:17 GMT</pubDate>
      <description>
      
        
        
          &lt;p&gt;最近从新开始学习机器学习，在看《统计学习方法》一书，其中提到：&lt;strong&gt;从贝叶斯估计的角度来看，正则化项对应于模型的先验概率。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;然后发现自己无法理解这句话的意思，于是，求助于google，找到了一篇写的还不错的博客：[从贝叶斯的角度来看
        
      
      </description>
      
      <content:encoded><![CDATA[<p>最近从新开始学习机器学习，在看《统计学习方法》一书，其中提到：<strong>从贝叶斯估计的角度来看，正则化项对应于模型的先验概率。</strong></p><p>然后发现自己无法理解这句话的意思，于是，求助于google，找到了一篇写的还不错的博客：[从贝叶斯的角度来看，正则化等价于对模型参数引入先验分布]<a href="https://www.zhihu.com/question/23536142/answer/90135994" target="_blank" rel="noopener">https://www.zhihu.com/question/23536142/answer/90135994</a></p><p>但是看上面那篇博客时，我发现，这篇文中有也很多我不懂的地方，首先我需要知道什么是贝叶斯回归…于是，继续google，然后，找到了另一篇通俗易懂的博客，分析了我们常用的频率线性回归和贝叶斯线性回归：[Introduction to Bayesian Linear Regression]<a href="https://towardsdatascience.com/introduction-to-bayesian-linear-regression-e66e60791ea7" target="_blank" rel="noopener">https://towardsdatascience.com/introduction-to-bayesian-linear-regression-e66e60791ea7</a></p><hr><p>总之，感受到自己的理论基础太差了，要学的东西很多；尤其是数学这一块，一直是我最头痛的地方，但是，还是要认真学啊…</p>]]></content:encoded>
      
      <comments>http://yoursite.com/2018/12/14/regularization/#disqus_thread</comments>
    </item>
    
    <item>
      <title>yolov3训练可视化</title>
      <link>http://yoursite.com/2018/12/05/visuialization/</link>
      <guid>http://yoursite.com/2018/12/05/visuialization/</guid>
      <pubDate>Wed, 05 Dec 2018 02:24:38 GMT</pubDate>
      <description>
      
        
        
          &lt;p&gt;这里记录一下训练yolov3时将输出记录并可视化的整个过程~&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&quot;将Terminal输出到文本文件&quot;&gt;&lt;a href=&quot;#将Terminal输出到文本文件&quot; class=&quot;headerlink&quot; title=&quot;将Terminal输出到文本文件&quot;
        
      
      </description>
      
      <content:encoded><![CDATA[<p>这里记录一下训练yolov3时将输出记录并可视化的整个过程~</p><hr><h2 id="将Terminal输出到文本文件"><a href="#将Terminal输出到文本文件" class="headerlink" title="将Terminal输出到文本文件"></a>将Terminal输出到文本文件</h2><p><img src="https://upload-images.jianshu.io/upload_images/14184962-15b455a8e41364f9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>说明：”&lt;”将输出转向，将命令行输出写入文本文件，并会清空原文件中的所有内容；而用”&lt;&lt;”则会将输出附加到文件后面，原内容保留。</p><p>但是这样命令行就看不到输出结果了。要在输出结果的同时将信息记录在文件中，则需使用命令<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">![](https://upload-images.jianshu.io/upload_images/14184962-0289fc45340a45f7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)</span><br><span class="line"></span><br><span class="line">该命令从标准输入读取内容并将其写到标准输出和文件中。</span><br><span class="line">***</span><br><span class="line">以上是在服务器训练时常用的。而对于一个计算平台，当我需要训练一个网络时，作为普通用户，我是在登录节点用自己的id登录，然后部署自己所需要的环境。当我把环境配置好了，需要跑网络时，就不能直接在该登录节点跑，这样会大量占用别人的资源。这时，需要递交一个作业到计算节点，利用GPU群进行高性能计算。一般是递交一个.sh文件，将所有命令都写在文件中。</span><br><span class="line"></span><br><span class="line">那么这个时候网络在计算节点训练，我在登录节点是没法看到输出结果的，但是我将输出都写入了日志文件。那么这个时候我想查看输出情况，该怎么办？对于一个不断更新的日志文件，如果直接打开对其进行读操作是会破坏其写入过程的，这个时候就需要用到```tail```命令了。</span><br><span class="line"></span><br><span class="line">```tail -f log.879.out</span><br></pre></td></tr></table></figure></p><p>这个命令会输出日志的最后十行，<code>-f</code>使其不断滚动更新，这样，我就可以监视输出结果了。</p><hr><h2 id="日志的可视化"><a href="#日志的可视化" class="headerlink" title="日志的可视化"></a>日志的可视化</h2>]]></content:encoded>
      
      <comments>http://yoursite.com/2018/12/05/visuialization/#disqus_thread</comments>
    </item>
    
    <item>
      <title>train yolov3</title>
      <link>http://yoursite.com/2018/12/03/train_yolov3/</link>
      <guid>http://yoursite.com/2018/12/03/train_yolov3/</guid>
      <pubDate>Mon, 03 Dec 2018 05:17:46 GMT</pubDate>
      <description>
      
        
        
          &lt;p&gt;这些天在复现YOLOV3时，训练过程中遇到了一些问题，在这里记录一下跑网络的过程~&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&quot;训练指令&quot;&gt;&lt;a href=&quot;#训练指令&quot; class=&quot;headerlink&quot; title=&quot;训练指令&quot;&gt;&lt;/a&gt;训练指令&lt;/h2&gt;&lt;p&gt;注意根据情况修改
        
      
      </description>
      
      <content:encoded><![CDATA[<p>这些天在复现YOLOV3时，训练过程中遇到了一些问题，在这里记录一下跑网络的过程~</p><hr><h2 id="训练指令"><a href="#训练指令" class="headerlink" title="训练指令"></a>训练指令</h2><p>注意根据情况修改Makefile文件，并重新make，以及cfg文件夹下yolov3-voc.cfg文件，在darknet目录下执行命令：</p><p>格式：<code>./darknet detector train [data_cfg] [network_cfg] [weight] -gpus [gpu_nums]</code></p><p><strong>CPU上训练</strong>：</p><p><code>./darknet -ngpu detector train cfg/voc.data cfg/yolov3-voc.cfg darknet.conv.74</code></p><p><strong>多GPU训练</strong>：</p><p><code>./darknet detector train cfg/voc.data cfg/yolov3-voc.cfg darknet.conv.74 -gpus 0,1</code></p><p><strong>输出重定向</strong></p><p>如果需要将训练过程可视化，则训练时将输出重定向得到训练日志文件；</p><p><code>./darknet detector train cfg/voc.data cfg/yolov3-voc.cfg darknet.conv.74 -gpus 0,1 2&gt;1 | tee train_yolov3.log</code></p><p><strong>从检查点开始恢复训练</strong></p><p><code>./darknet detector train cfg/voc.data cfg/yolov3-voc.cfg backup/yolov3.backup -gpus 0,1</code></p><hr><h2 id="输出解释"><a href="#输出解释" class="headerlink" title="输出解释"></a>输出解释</h2><h3 id="forward-yolo-layer"><a href="#forward-yolo-layer" class="headerlink" title="forward_yolo_layer()"></a><em>forward_yolo_layer()</em></h3><p>每个子batch输出一次，详见src文件夹下yolo_layer.c文件的<em>forward_yolo_layer</em>函数。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">printf(&quot;Region %d Avg IOU: %f, Class: %f, Obj: %f, No Obj: %f, .5R: %f, .75R: %f,  count: %d\n&quot;, net.index, avg_iou/count, avg_cat/class_count, avg_obj/count, avg_anyobj/(l.w*l.h*l.n*l.batch), recall/count, recall75/count, count);</span><br></pre></td></tr></table></figure><ul><li><p><code>Region [xx]</code>: cfg文件中的layer的索引；</p></li><li><p><code>Avg IOU</code>: 当前迭代过程中，预测的bbox和ground truth的平均交并比，理想值为1；</p></li><li><p><code>Class</code>: 标注物体的分类准确率，理想值为1；</p></li><li><p><code>Obj</code>: 期望该值越接近1越好；</p></li><li><p><code>No Obj</code>: 期望该值越小越好；</p></li><li><p><code>.5R</code>: IoU=0.5为阈值时，当前子batch中recall值；</p></li><li><p><code>.75R</code>: IoU=0.75为阈值时，当前子batch中recall值；</p></li><li><p><code>count</code>: 当前subdivision的子batch中所有图片中包含的正样本数。<code>count = 0</code>表示该批次该尺度上没有检测到对象，会导致<code>Avg IOU</code>、<code>Class</code>、<code>obj</code>、<code>.5R</code>、<code>.75R</code>值均为<code>-nan</code>或<code>nan</code>。这可能是由于在许多小物体上训练导致的，所以在显存允许的情况下，可适当增大<code>batch</code>的大小，可一定程度减少nan的出现。</p></li></ul><h3 id="train-detector"><a href="#train-detector" class="headerlink" title="train_detector()"></a><em>train_detector()</em></h3><p>每个batch输出一次loss，详见examples文件夹下detect.c文件中<em>train_detector</em>函数:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">printf(&quot;%ld: %f, %f avg, %f rate, %lf seconds, %d images\n&quot;, get_current_batch(net), loss, avg_loss, get_current_rate(net), what_time_is_it_now()-time, i*imgs);</span><br></pre></td></tr></table></figure><hr><h2 id="问题及解决"><a href="#问题及解决" class="headerlink" title="问题及解决"></a>问题及解决</h2><h3 id="CUDA-out-of-memory"><a href="#CUDA-out-of-memory" class="headerlink" title="CUDA: out of memory"></a>CUDA: out of memory</h3><p>显存不够引起的，修改yolov3-voc.cfg文件。</p><ul><li><p>修改<code>[net]</code>层参数，增大<code>subdivisions</code>或调小<code>batch</code>，因为训练时实际每次前向的图片数量 = batch/subdivisions。</p></li><li><p>关闭多尺度训练。将所有<code>[yolo]</code>层中<code>random</code>参数修改为<code>random = 0</code>。</p></li></ul><hr><h2 id="参数调整"><a href="#参数调整" class="headerlink" title="参数调整"></a>参数调整</h2><ol><li><p>Batch_Size：想要模型快速收敛，最好是增大Batch_Size，但可能导致陷入局部最小的情况；减小Batch_Size可能会有更好的效果，但是如果类别数较多，Batch_Size又太小，会导致loss函数震荡不收敛。一般在调试时，根据GPU显存，设置为最大（一般要求为8的倍数），选部分数据跑几个Batch看看loss是否在变小，再选择合适的值。yolov3-voc.cfg文件中对应参数为<code>batch</code>。</p></li><li><p>如果内存不够大，增大<code>subdivision</code>值，将batch分割，每个子batch大小为 batch/subdivisions。</p></li><li><p>为了应对小目标检测，网络输入大一些更好,即增大<code>weight</code>和<code>height</code>。</p></li><li><p><code>random = 1</code>会使用多尺度训练，随机使用不同尺寸的图片进行训练；如果为0，则每次训练图片大小与输入尺寸一致。</p></li></ol>]]></content:encoded>
      
      <comments>http://yoursite.com/2018/12/03/train_yolov3/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Linux下监视NVIDIA的GPU使用情况</title>
      <link>http://yoursite.com/2018/12/02/watch_gpu/</link>
      <guid>http://yoursite.com/2018/12/02/watch_gpu/</guid>
      <pubDate>Sun, 02 Dec 2018 10:20:37 GMT</pubDate>
      <description>
      
        
        
          &lt;p&gt;最近在服务器上跑网络，发现对命令行还是很不熟悉，每次都记不住要靠google，有很多东西要学。还是趁这个机会边学边整理吧。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&quot;显示当前GPU使用情况&quot;&gt;&lt;a href=&quot;#显示当前GPU使用情况&quot; class=&quot;headerlink&quot; tit
        
      
      </description>
      
      <content:encoded><![CDATA[<p>最近在服务器上跑网络，发现对命令行还是很不熟悉，每次都记不住要靠google，有很多东西要学。还是趁这个机会边学边整理吧。</p><hr><h2 id="显示当前GPU使用情况"><a href="#显示当前GPU使用情况" class="headerlink" title="显示当前GPU使用情况"></a>显示当前GPU使用情况</h2><p>在命令行下，Nvidia自带了工具，会显示显存使用情况。命令如下：</p><p><code>$ nvidia-smi</code></p><p>输出如图：</p><p><img src="https://upload-images.jianshu.io/upload_images/14184962-f2a374580f44f847.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><h2 id="nvdia-smi表含义"><a href="#nvdia-smi表含义" class="headerlink" title="nvdia-smi表含义"></a>nvdia-smi表含义</h2><p>表中第一行是显卡版本信息，第二行是标题栏，对应含义如下；</p><p><strong>GPU</strong>：显卡编号<br><strong>Fan</strong>：风扇转速，范围在0~100%<br><strong>Name</strong>：显卡名，这里是四张Geforce GTX 1080Ti<br><strong>Temp</strong>：显卡温度，单位摄氏度<br><strong>Perf</strong>：显卡性能，P0~P12，P0表示最大性能<br><strong>Persistence-M</strong>：持续模式的状态，该模式耗能大，但是在新的GPU启动时，花费的时间更少；这里是off状态<br><strong>Pwr：Usage/Cap</strong>：能耗，当前能耗/最大能耗,单位瓦</p><p><strong>Bus-Id</strong>：GPU总线的相关信息，domain:bus:device.function<br><strong>Disp.A</strong>：GPU的显示是否初始化，display activate<br><strong>Memory-Usage</strong>：显存使用情况，当前占用显存/总显存</p><p><strong>Uncorr. ECC</strong>：关于ECC的信息<br><strong>Volatile GPU-Util</strong>：浮动的GPU利用率<br><strong>Compute M.</strong>：计算模式，这里是默认模式</p><p>表格下半部分显示的是每个显卡上相应进程的显存占用情况。</p><hr><p><code>$ nvidia-smi -h</code></p><p>通过该命令可获得mvidia-smi系统管理界面所有相关信息。</p><hr><h2 id="周期性更新GPU状态"><a href="#周期性更新GPU状态" class="headerlink" title="周期性更新GPU状态"></a>周期性更新GPU状态</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ whatis watch</span><br><span class="line">watch(1)        - execute a program periodically, showing output fullscreen</span><br></pre></td></tr></table></figure><p><code>$ watch [options] nvidia-smi</code></p><p>options为可选参数，通常用<code>-n</code>，后接数字表示多少秒执行一次命令。</p><p><code>$ watch -n 20 nvidia-smi</code></p><p>会进入监视界面，并且每20秒会刷新一次显卡状态，退出监视模式只要按<code>Ctrl+z</code>。</p>]]></content:encoded>
      
      <comments>http://yoursite.com/2018/12/02/watch_gpu/#disqus_thread</comments>
    </item>
    
    <item>
      <title>a virtualenv on linux</title>
      <link>http://yoursite.com/2018/12/01/virtualenv/</link>
      <guid>http://yoursite.com/2018/12/01/virtualenv/</guid>
      <pubDate>Sat, 01 Dec 2018 06:43:22 GMT</pubDate>
      <description>
      
        
        
          &lt;p&gt;前些天在实验室的服务器上部署环境跑网络。然后，自己总是忘记怎么进入自己搭建的虚拟环境，不得已在这里记录一下…&lt;/p&gt;
&lt;p&gt;在自己的笔记本上安装了Anaconda，不过服务器上没有conda，只安装了python-virtualenv，不过效果也差不多。&lt;/p&gt;
&lt;p&gt;例如
        
      
      </description>
      
      <content:encoded><![CDATA[<p>前些天在实验室的服务器上部署环境跑网络。然后，自己总是忘记怎么进入自己搭建的虚拟环境，不得已在这里记录一下…</p><p>在自己的笔记本上安装了Anaconda，不过服务器上没有conda，只安装了python-virtualenv，不过效果也差不多。</p><p>例如：</p><p><strong>创建名为’py3’的虚拟环境</strong></p><p><code>virtualenv py3</code></p><p>默认情况下，虚拟环境会依赖系统环境中的site packages，如果不需要系统环境中的这些第三方包，则需要在上述命令后加上参数 <code>--no-site-packages</code> 来创建虚拟环境。</p><p><strong>启动虚拟环境</strong></p><p><code>cd py3</code><br><code>source ./bin/activate</code></p><p><strong>退出虚拟环境</strong></p><p><code>deactivate</code></p><p>启动虚拟环境后，所有通过pip安装的模块都会被安装在该环境中，不会对系统环境产生影响。要删除该虚拟环境，只需要将相应文件夹删除即可。</p><hr><p>那么在安装了Anaconda时，创建独立的环境命令如下（其中tf为自己取的虚拟环境名称）：</p><p><code>conda create -n tf</code></p><p><strong>进入虚拟环境</strong></p><p><code>source activate tf</code></p><p><strong>安装需要的模块</strong></p><p><code>conda install tensorflow-gpu</code></p><p><strong>列出环境中所有已装的模块</strong></p><p><code>conda list</code></p><p><strong>退出虚拟环境</strong></p><p><code>source deactivate</code></p><p><strong>列出所有的环境(包括base)</strong></p><p><code>conda env list</code></p><p><strong>删除某个虚拟环境</strong></p><p><code>conda remove -n tf -all</code></p>]]></content:encoded>
      
      <comments>http://yoursite.com/2018/12/01/virtualenv/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Caffe &quot;in-place&quot; operation</title>
      <link>http://yoursite.com/2018/11/29/caffe_inplace/</link>
      <guid>http://yoursite.com/2018/11/29/caffe_inplace/</guid>
      <pubDate>Thu, 29 Nov 2018 02:48:25 GMT</pubDate>
      <description>
      
        
        
          &lt;p&gt;在学习Caffe model中关于Layers内容时，发现一个问题，ReLU层的top blob和bottom blob相同，如下：&lt;br&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;
        
      
      </description>
      
      <content:encoded><![CDATA[<p>在学习Caffe model中关于Layers内容时，发现一个问题，ReLU层的top blob和bottom blob相同，如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">layer &#123;</span><br><span class="line">  name: &quot;conv2/relu_3x3_reduce&quot;</span><br><span class="line">  type: &quot;ReLU&quot;</span><br><span class="line">  bottom: &quot;conv2/3x3_reduce&quot;</span><br><span class="line">  top: &quot;conv2/3x3_reduce&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>后来查资料发现这是caffe的in-place操作，是为了节省内（显）存，以及省去反复申请和释放内存的时间。</p><p>bottom blob和top blob名称相同，说明是同一个blob，占用的是内存的同一块空间。在这里也就是说，该ReLU操作是对输入blob本身进行操作，再将其输出。</p><p>如果定义的两个<code>layer</code>的top blob名称是一样的，那么这两个<code>layer</code>的bottom blob也一定是该top blob，并且按<code>layer</code>的定义顺序对该bottom blob进行操作。如果layer不是对bottom blob本身进行操作，那么top blob就不能与bottom blob相同，也不允许多个layer的top blob同名。因为假如这样做，后运算的layer会将top blob覆盖成其运算的结果，前面的layer的运算结果就消失了。</p><p>convolution、pooling层由于输入输出的size一般不一致，所以不能支持in-place操作；而ReLU函数是逐个对元素进行计算，不该变size，所以支持。AlexNet、VGGnet等都是在ReLU层使用in-place计算，ResNet中，BatchNorm和Scale也都使用了in-place计算。</p><p>目前已知支持in-place的有：ReLU层、Dropout层、BatchNorm层、Scale层。(除此之外，其他大部分layer貌似在定义时<code>name</code>和<code>top</code>是相同的。)</p>]]></content:encoded>
      
      <comments>http://yoursite.com/2018/11/29/caffe_inplace/#disqus_thread</comments>
    </item>
    
  </channel>
</rss>
