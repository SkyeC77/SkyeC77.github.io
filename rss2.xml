<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"
  xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Notes</title>
    <link>http://yoursite.com/</link>
    <atom:link href="/rss2.xml" rel="self" type="application/rss+xml"/>
    
    <description></description>
    <pubDate>Wed, 05 Dec 2018 03:13:56 GMT</pubDate>
    <generator>http://hexo.io/</generator>
    
    <item>
      <title>yolov3训练可视化</title>
      <link>http://yoursite.com/2018/12/05/visuialization/</link>
      <guid>http://yoursite.com/2018/12/05/visuialization/</guid>
      <pubDate>Wed, 05 Dec 2018 02:24:38 GMT</pubDate>
      <description>
      
        
        
          &lt;p&gt;这里记录一下训练yolov3时将输出记录并可视化的整个过程~&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&quot;将Terminal输出到文本文件&quot;&gt;&lt;a href=&quot;#将Terminal输出到文本文件&quot; class=&quot;headerlink&quot; title=&quot;将Terminal输出到文本文件&quot;
        
      
      </description>
      
      <content:encoded><![CDATA[<p>这里记录一下训练yolov3时将输出记录并可视化的整个过程~</p><hr><h2 id="将Terminal输出到文本文件"><a href="#将Terminal输出到文本文件" class="headerlink" title="将Terminal输出到文本文件"></a>将Terminal输出到文本文件</h2><p><img src="https://upload-images.jianshu.io/upload_images/14184962-15b455a8e41364f9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>说明：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">但是这样命令行就看不到输出结果了。要在输出结果的同时将信息记录在文件中，则需使用命令：```tee```，从标准输入读取内容并将其写到标准输出和文件中。</span><br><span class="line"></span><br><span class="line">![](https://upload-images.jianshu.io/upload_images/14184962-0289fc45340a45f7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)</span><br><span class="line"></span><br><span class="line">***</span><br><span class="line">以上是在服务器训练时常用的。而对于一个计算平台，当我需要训练一个网络时，作为普通用户，我是在登录节点用自己的id登录，然后部署自己所需要的环境。当我把环境配置好了，需要跑网络时，就不能直接在该登录节点跑，这样会大量占用别人的资源。这时，需要递交一个作业到计算节点，利用GPU群进行高性能计算。一般是递交一个.sh文件，将所有命令都写在文件中。</span><br><span class="line"></span><br><span class="line">那么这个时候网络在计算节点训练，我在登录节点是没法看到输出结果的，但是我将输出都写入了日志文件。那么这个时候我想查看输出情况，该怎么办？对于一个不断更新的日志文件，如果直接打开对其进行读操作是会破坏其写入过程的，这个时候就需要用到```tail```命令了：</span><br><span class="line"></span><br><span class="line">```tail -f log.879.out</span><br></pre></td></tr></table></figure></p><p>这个命令会输出日志的最后十行，<code>-f</code>使其不断滚动更新，这样，我就可以监视输出结果了。</p><hr><h2 id="日志的可视化"><a href="#日志的可视化" class="headerlink" title="日志的可视化"></a>日志的可视化</h2>]]></content:encoded>
      
      <comments>http://yoursite.com/2018/12/05/visuialization/#disqus_thread</comments>
    </item>
    
    <item>
      <title>train yolov3</title>
      <link>http://yoursite.com/2018/12/03/train_yolov3/</link>
      <guid>http://yoursite.com/2018/12/03/train_yolov3/</guid>
      <pubDate>Mon, 03 Dec 2018 05:17:46 GMT</pubDate>
      <description>
      
        
        
          &lt;p&gt;这些天在复现YOLOV3时，训练过程中遇到了一些问题，在这里记录一下跑网络的过程~&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&quot;训练指令&quot;&gt;&lt;a href=&quot;#训练指令&quot; class=&quot;headerlink&quot; title=&quot;训练指令&quot;&gt;&lt;/a&gt;训练指令&lt;/h2&gt;&lt;p&gt;注意根据情况修改
        
      
      </description>
      
      <content:encoded><![CDATA[<p>这些天在复现YOLOV3时，训练过程中遇到了一些问题，在这里记录一下跑网络的过程~</p><hr><h2 id="训练指令"><a href="#训练指令" class="headerlink" title="训练指令"></a>训练指令</h2><p>注意根据情况修改Makefile文件，并重新make，以及cfg文件夹下yolov3-voc.cfg文件，在darknet目录下执行命令：</p><p>格式：<code>./darknet detector train [data_cfg] [network_cfg] [weight] -gpus [gpu_nums]</code></p><p><strong>CPU上训练</strong>：</p><p><code>./darknet -ngpu detector train cfg/voc.data cfg/yolov3-voc.cfg darknet.conv.74</code></p><p><strong>多GPU训练</strong>：</p><p><code>./darknet detector train cfg/voc.data cfg/yolov3-voc.cfg darknet.conv.74 -gpus 0,1</code></p><p><strong>输出重定向</strong></p><p>如果需要将训练过程可视化，则训练时将输出重定向得到训练日志文件；</p><p><code>./darknet detector train cfg/voc.data cfg/yolov3-voc.cfg darknet.conv.74 -gpus 0,1 2&gt;1 | tee train_yolov3.log</code></p><p><strong>从检查点开始恢复训练</strong></p><p><code>./darknet detector train cfg/voc.data cfg/yolov3-voc.cfg backup/yolov3.backup -gpus 0,1</code></p><hr><h2 id="输出解释"><a href="#输出解释" class="headerlink" title="输出解释"></a>输出解释</h2><h3 id="forward-yolo-layer"><a href="#forward-yolo-layer" class="headerlink" title="forward_yolo_layer()"></a><em>forward_yolo_layer()</em></h3><p>每个子batch输出一次，详见src文件夹下yolo_layer.c文件的<em>forward_yolo_layer</em>函数。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">printf(&quot;Region %d Avg IOU: %f, Class: %f, Obj: %f, No Obj: %f, .5R: %f, .75R: %f,  count: %d\n&quot;, net.index, avg_iou/count, avg_cat/class_count, avg_obj/count, avg_anyobj/(l.w*l.h*l.n*l.batch), recall/count, recall75/count, count);</span><br></pre></td></tr></table></figure><ul><li><p><code>Region [xx]</code>: cfg文件中的layer的索引；</p></li><li><p><code>Avg IOU</code>: 当前迭代过程中，预测的bbox和ground truth的平均交并比，理想值为1；</p></li><li><p><code>Class</code>: 标注物体的分类准确率，理想值为1；</p></li><li><p><code>Obj</code>: 期望该值越接近1越好；</p></li><li><p><code>No Obj</code>: 期望该值越小越好；</p></li><li><p><code>.5R</code>: IoU=0.5为阈值时，当前子batch中recall值；</p></li><li><p><code>.75R</code>: IoU=0.75为阈值时，当前子batch中recall值；</p></li><li><p><code>count</code>: 当前subdivision的子batch中所有图片中包含的正样本数。<code>count = 0</code>表示该批次该尺度上没有检测到对象，会导致<code>Avg IOU</code>、<code>Class</code>、<code>obj</code>、<code>.5R</code>、<code>.75R</code>值均为<code>-nan</code>或<code>nan</code>。这可能是由于在许多小物体上训练导致的，所以在显存允许的情况下，可适当增大<code>batch</code>的大小，可一定程度减少nan的出现。</p></li></ul><h3 id="train-detector"><a href="#train-detector" class="headerlink" title="train_detector()"></a><em>train_detector()</em></h3><p>每个batch输出一次loss，详见examples文件夹下detect.c文件中<em>train_detector</em>函数:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">printf(&quot;%ld: %f, %f avg, %f rate, %lf seconds, %d images\n&quot;, get_current_batch(net), loss, avg_loss, get_current_rate(net), what_time_is_it_now()-time, i*imgs);</span><br></pre></td></tr></table></figure><hr><h2 id="问题及解决"><a href="#问题及解决" class="headerlink" title="问题及解决"></a>问题及解决</h2><h3 id="CUDA-out-of-memory"><a href="#CUDA-out-of-memory" class="headerlink" title="CUDA: out of memory"></a>CUDA: out of memory</h3><p>显存不够引起的，修改yolov3-voc.cfg文件。</p><ul><li><p>修改<code>[net]</code>层参数，增大<code>subdivisions</code>或调小<code>batch</code>，因为训练时实际每次前向的图片数量 = batch/subdivisions。</p></li><li><p>关闭多尺度训练。将所有<code>[yolo]</code>层中<code>random</code>参数修改为<code>random = 0</code>。</p></li></ul><hr><h2 id="参数调整"><a href="#参数调整" class="headerlink" title="参数调整"></a>参数调整</h2><ol><li><p>Batch_Size：想要模型快速收敛，最好是增大Batch_Size，但可能导致陷入局部最小的情况；减小Batch_Size可能会有更好的效果，但是如果类别数较多，Batch_Size又太小，会导致loss函数震荡不收敛。一般在调试时，根据GPU显存，设置为最大（一般要求为8的倍数），选部分数据跑几个Batch看看loss是否在变小，再选择合适的值。yolov3-voc.cfg文件中对应参数为<code>batch</code>。</p></li><li><p>如果内存不够大，增大<code>subdivision</code>值，将batch分割，每个子batch大小为 batch/subdivisions。</p></li><li><p>为了应对小目标检测，网络输入大一些更好,即增大<code>weight</code>和<code>height</code>。</p></li><li><p><code>random = 1</code>会使用多尺度训练，随机使用不同尺寸的图片进行训练；如果为0，则每次训练图片大小与输入尺寸一致。</p></li></ol>]]></content:encoded>
      
      <comments>http://yoursite.com/2018/12/03/train_yolov3/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Linux下监视NVIDIA的GPU使用情况</title>
      <link>http://yoursite.com/2018/12/02/watch_gpu/</link>
      <guid>http://yoursite.com/2018/12/02/watch_gpu/</guid>
      <pubDate>Sun, 02 Dec 2018 10:20:37 GMT</pubDate>
      <description>
      
        
        
          &lt;p&gt;最近在服务器上跑网络，发现对命令行还是很不熟悉，每次都记不住要靠google，有很多东西要学。还是趁这个机会边学边整理吧。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&quot;显示当前GPU使用情况&quot;&gt;&lt;a href=&quot;#显示当前GPU使用情况&quot; class=&quot;headerlink&quot; tit
        
      
      </description>
      
      <content:encoded><![CDATA[<p>最近在服务器上跑网络，发现对命令行还是很不熟悉，每次都记不住要靠google，有很多东西要学。还是趁这个机会边学边整理吧。</p><hr><h2 id="显示当前GPU使用情况"><a href="#显示当前GPU使用情况" class="headerlink" title="显示当前GPU使用情况"></a>显示当前GPU使用情况</h2><p>在命令行下，Nvidia自带了工具，会显示显存使用情况。命令如下：</p><p><code>$ nvidia-smi</code></p><p>输出如图：</p><p><img src="https://upload-images.jianshu.io/upload_images/14184962-f2a374580f44f847.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><h2 id="nvdia-smi表含义"><a href="#nvdia-smi表含义" class="headerlink" title="nvdia-smi表含义"></a>nvdia-smi表含义</h2><p>表中第一行是显卡版本信息，第二行是标题栏，对应含义如下；</p><p><strong>GPU</strong>：显卡编号<br><strong>Fan</strong>：风扇转速，范围在0~100%<br><strong>Name</strong>：显卡名，这里是四张Geforce GTX 1080Ti<br><strong>Temp</strong>：显卡温度，单位摄氏度<br><strong>Perf</strong>：显卡性能，P0~P12，P0表示最大性能<br><strong>Persistence-M</strong>：持续模式的状态，该模式耗能大，但是在新的GPU启动时，花费的时间更少；这里是off状态<br><strong>Pwr：Usage/Cap</strong>：能耗，当前能耗/最大能耗,单位瓦</p><p><strong>Bus-Id</strong>：GPU总线的相关信息，domain:bus:device.function<br><strong>Disp.A</strong>：GPU的显示是否初始化，display activate<br><strong>Memory-Usage</strong>：显存使用情况，当前占用显存/总显存</p><p><strong>Uncorr. ECC</strong>：关于ECC的信息<br><strong>Volatile GPU-Util</strong>：浮动的GPU利用率<br><strong>Compute M.</strong>：计算模式，这里是默认模式</p><p>表格下半部分显示的是每个显卡上相应进程的显存占用情况。</p><hr><p><code>$ nvidia-smi -h</code></p><p>通过该命令可获得mvidia-smi系统管理界面所有相关信息。</p><hr><h2 id="周期性更新GPU状态"><a href="#周期性更新GPU状态" class="headerlink" title="周期性更新GPU状态"></a>周期性更新GPU状态</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ whatis watch</span><br><span class="line">watch(1)        - execute a program periodically, showing output fullscreen</span><br></pre></td></tr></table></figure><p><code>$ watch [options] nvidia-smi</code></p><p>options为可选参数，通常用<code>-n</code>，后接数字表示多少秒执行一次命令。</p><p><code>$ watch -n 20 nvidia-smi</code></p><p>会进入监视界面，并且每20秒会刷新一次显卡状态，退出监视模式只要按<code>Ctrl+z</code>。</p>]]></content:encoded>
      
      <comments>http://yoursite.com/2018/12/02/watch_gpu/#disqus_thread</comments>
    </item>
    
    <item>
      <title>a virtualenv on linux</title>
      <link>http://yoursite.com/2018/12/01/virtualenv/</link>
      <guid>http://yoursite.com/2018/12/01/virtualenv/</guid>
      <pubDate>Sat, 01 Dec 2018 06:43:22 GMT</pubDate>
      <description>
      
        
        
          &lt;p&gt;前些天在实验室的服务器上部署环境跑网络。然后，自己总是忘记怎么进入自己搭建的虚拟环境，不得已在这里记录一下…&lt;/p&gt;
&lt;p&gt;在自己的笔记本上安装了Anaconda，不过服务器上没有conda，只安装了python-virtualenv，不过效果也差不多。&lt;/p&gt;
&lt;p&gt;例如
        
      
      </description>
      
      <content:encoded><![CDATA[<p>前些天在实验室的服务器上部署环境跑网络。然后，自己总是忘记怎么进入自己搭建的虚拟环境，不得已在这里记录一下…</p><p>在自己的笔记本上安装了Anaconda，不过服务器上没有conda，只安装了python-virtualenv，不过效果也差不多。</p><p>例如：</p><p><strong>创建名为’py3’的虚拟环境</strong></p><p><code>virtualenv py3</code></p><p>默认情况下，虚拟环境会依赖系统环境中的site packages，如果不需要系统环境中的这些第三方包，则需要在上述命令后加上参数 <code>--no-site-packages</code> 来创建虚拟环境。</p><p><strong>启动虚拟环境</strong></p><p><code>cd py3</code><br><code>source ./bin/activate</code></p><p><strong>退出虚拟环境</strong></p><p><code>deactivate</code></p><p>启动虚拟环境后，所有通过pip安装的模块都会被安装在该环境中，不会对系统环境产生影响。要删除该虚拟环境，只需要将相应文件夹删除即可。</p><hr><p>那么在安装了Anaconda时，创建独立的环境命令如下（其中tf为自己取的虚拟环境名称）：</p><p><code>conda create -n tf</code></p><p><strong>进入虚拟环境</strong></p><p><code>source activate tf</code></p><p><strong>安装需要的模块</strong></p><p><code>conda install tensorflow-gpu</code></p><p><strong>列出环境中所有已装的模块</strong></p><p><code>conda list</code></p><p><strong>退出虚拟环境</strong></p><p><code>source deactivate</code></p><p><strong>列出所有的环境(包括base)</strong></p><p><code>conda env list</code></p><p><strong>删除某个虚拟环境</strong></p><p><code>conda remove -n tf -all</code></p>]]></content:encoded>
      
      <comments>http://yoursite.com/2018/12/01/virtualenv/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Caffe &quot;in-place&quot; operation</title>
      <link>http://yoursite.com/2018/11/29/caffe_inplace/</link>
      <guid>http://yoursite.com/2018/11/29/caffe_inplace/</guid>
      <pubDate>Thu, 29 Nov 2018 02:48:25 GMT</pubDate>
      <description>
      
        
        
          &lt;p&gt;在学习Caffe model中关于Layers内容时，发现一个问题，ReLU层的top blob和bottom blob相同，如下：&lt;br&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;
        
      
      </description>
      
      <content:encoded><![CDATA[<p>在学习Caffe model中关于Layers内容时，发现一个问题，ReLU层的top blob和bottom blob相同，如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">layer &#123;</span><br><span class="line">  name: &quot;conv2/relu_3x3_reduce&quot;</span><br><span class="line">  type: &quot;ReLU&quot;</span><br><span class="line">  bottom: &quot;conv2/3x3_reduce&quot;</span><br><span class="line">  top: &quot;conv2/3x3_reduce&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>后来查资料发现这是caffe的in-place操作，是为了节省内（显）存，以及省去反复申请和释放内存的时间。</p><p>bottom blob和top blob名称相同，说明是同一个blob，占用的是内存的同一块空间。在这里也就是说，该ReLU操作是对输入blob本身进行操作，再将其输出。</p><p>如果定义的两个<code>layer</code>的top blob名称是一样的，那么这两个<code>layer</code>的bottom blob也一定是该top blob，并且按<code>layer</code>的定义顺序对该bottom blob进行操作。如果layer不是对bottom blob本身进行操作，那么top blob就不能与bottom blob相同，也不允许多个layer的top blob同名。因为假如这样做，后运算的layer会将top blob覆盖成其运算的结果，前面的layer的运算结果就消失了。</p><p>convolution、pooling层由于输入输出的size一般不一致，所以不能支持in-place操作；而ReLU函数是逐个对元素进行计算，不该变size，所以支持。AlexNet、VGGnet等都是在ReLU层使用in-place计算，ResNet中，BatchNorm和Scale也都使用了in-place计算。</p><p>目前已知支持in-place的有：ReLU层、Dropout层、BatchNorm层、Scale层。(除此之外，其他大部分layer貌似在定义时<code>name</code>和<code>top</code>是相同的。)</p>]]></content:encoded>
      
      <comments>http://yoursite.com/2018/11/29/caffe_inplace/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Yolov3 Network Architecture</title>
      <link>http://yoursite.com/2018/11/28/yolov3/</link>
      <guid>http://yoursite.com/2018/11/28/yolov3/</guid>
      <pubDate>Wed, 28 Nov 2018 09:59:59 GMT</pubDate>
      <description>
      
        
        
          &lt;p&gt;贴一张Yolov3网络结构图~&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://upload-images.jianshu.io/upload_images/14184962-7d6aca0fa7d27e52.jpg?imageMogr2/auto-orient/stri
        
      
      </description>
      
      <content:encoded><![CDATA[<p>贴一张Yolov3网络结构图~</p><p><img src="https://upload-images.jianshu.io/upload_images/14184962-7d6aca0fa7d27e52.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>filter = 3*(80+1+4) = 255</p><p>Yolov3 is a fully convolutional network. It has 75 convolutional layers, with skip connections and upsampling layers. No form of pooling is used, and a convolutional layer with stride 2 is used to downsample the feature maps, which helps in preventing loss of low-level features often attributed to pooling.</p><p>详细内容见：<a href="https://www.cyberailab.com/home/a-closer-look-at-yolov3" target="_blank" rel="noopener"><strong>A Closer Look at YOLOv3</strong></a></p>]]></content:encoded>
      
      <comments>http://yoursite.com/2018/11/28/yolov3/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Some notes for deep learning</title>
      <link>http://yoursite.com/2018/11/27/notes_for_dl/</link>
      <guid>http://yoursite.com/2018/11/27/notes_for_dl/</guid>
      <pubDate>Tue, 27 Nov 2018 15:41:55 GMT</pubDate>
      <description>
      
        
        
          &lt;h2 id=&quot;Object-localization&quot;&gt;&lt;a href=&quot;#Object-localization&quot; class=&quot;headerlink&quot; title=&quot;Object localization&quot;&gt;&lt;/a&gt;Object localization&lt;/h2&gt;&lt;p&gt;监督
        
      
      </description>
      
      <content:encoded><![CDATA[<h2 id="Object-localization"><a href="#Object-localization" class="headerlink" title="Object localization"></a>Object localization</h2><p>监督学习<br>训练样本中给出bounding box的中心点坐标和长宽，bx,by,bw,bh<br>Need to output bx,by,bw,bh,class label and Pc(is there any object?存在物体的置信度。当不存在物体的时候，loss函数只需计算Pc的准确度，其他项都不用考虑)。Y的维数为（1+4+class labels)。</p><hr><h2 id="Landmark-detection"><a href="#Landmark-detection" class="headerlink" title="Landmark detection"></a>Landmark detection</h2><p>特征点检测</p><p>需要输出一个是否有物体的置信度Pc,以及每个特征点的（x,y）坐标，所有训练集样本需包含这些labels。</p><p>面部表情识别，人体关键点检测。</p><hr><h2 id="Object-detection"><a href="#Object-detection" class="headerlink" title="Object detection"></a>Object detection</h2><h3 id="sliding-windows"><a href="#sliding-windows" class="headerlink" title="sliding windows"></a>sliding windows</h3><p>训练：根据滑动窗口的大小，使用适当裁剪的训练集样本，训练是否图片中有需要检测的物体物体。<br>检测：滑动窗口目标检测，对每一个窗口做一次检测，得到一个label,直到遍历图像每个区域。<br>缺点：computation expensive。步幅大时，粗粒度会影响检测性能，无法准确定位；细粒度或小步幅时计算成本高。所以一般使用简单的线性分类器。</p><h3 id="convolutional-implementation-of-sliding-windows"><a href="#convolutional-implementation-of-sliding-windows" class="headerlink" title="convolutional implementation of sliding windows"></a>convolutional implementation of sliding windows</h3><p>Turning FC layer into convolutional layers<br>没有全连接层时，输入图片的大小可以改变。<br>该卷积操作的原理是：不需要将需检测的输入图片分割成滑窗的大小的子集，分贝执行前向传播；而是直接将图片输入给卷积网络，其中的公共区域可以共享很多计算，一次性输出所有检测值。但边界框位置可能不够准确。</p><h3 id="bounding-box-prediction"><a href="#bounding-box-prediction" class="headerlink" title="bounding box prediction"></a>bounding box prediction</h3><p>YOLO algorithm</p><p>一个格子只存在一个物体时<br>用一个网格划分输入图片（3<em>3）<br>Labels for training for each grid cell:置信度Pc，bx,by,bw,bh, class labels<br>将物体分配给中心所在的格子(训练集中该格子的Pc=1)<br>output dimension:(1+4+class labels)\</em>3*3,<br>bx,by,bw,bh are specified relative to the grid cell(bbox中心所在cell的左上角坐标为0，右下角为1，bx,by是相对（0,0)的偏移量，值在0到1之间；bw,bh是bbox的长宽与grid cell边长的比值，可大于1。可以用sigmoid等函数处理，使其位于0到1之间）</p><h3 id="Intersection-over-Union"><a href="#Intersection-over-Union" class="headerlink" title="Intersection over Union"></a>Intersection over Union</h3><p>“Correct” if IoU&gt;=0.5(human chosen)<br>the predicted bbox with the ground truth(prior bbox)</p><h3 id="Non-max-Suppresion"><a href="#Non-max-Suppresion" class="headerlink" title="Non-max Suppresion"></a>Non-max Suppresion</h3><p>确保每个物体只被检测出一次。<br>先去掉所有Pc小于一定阈值的边界框(认为没有检测到物体），找出检测结果中Pc最大的bbox,然后去掉与其IoU大于阈值的检测框；再选择剩下的bbox中概率最高的…直到处理完所有框。<br>如果图像中有多中类别的物体，应该分别对每种label各<strong>独立</strong>做一次NMS。</p><hr><h3 id="Anchor-box"><a href="#Anchor-box" class="headerlink" title="Anchor box"></a>Anchor box</h3><p>使一个grid cell可以检测出多个物体。</p><p><strong>Previously</strong>:<br>each object in training image is assigned to grid cell that contains that object’s midpoint.</p><p><strong>With anchor boxes</strong>:<br>each object in training image is assigned to grid cell that contains object’s midpoint and anchor box for the grid cell with highest IoU. match with <strong>(grid cell, anchor box)</strong>.</p><p>for each grid cell, the output Y dimension is (1+4+class labels)*anchor boxes.</p><p>use k-means algorithm to choose anchor box.</p><p>grid cells越多，一个cell中出现多个物体的可能性更小。</p><hr><h2 id="Region-proposal-network-RPN"><a href="#Region-proposal-network-RPN" class="headerlink" title="Region proposal network(RPN)"></a>Region proposal network(RPN)</h2><p>two stages<br>利用图像分割算法，选出候选区域，在每种色块(blobs)上跑分类器。先找出可能的2000个色块，然后在这些色块上放置bbox，在这些色块上跑分类器。</p><p>R-CNN: Propose regions. Classify proposed regions one at a time. Output label + bounding box.</p><p>Fast R-CNN: Propose regions. Useconvolution implementation of sliding windows to classify all the proposed regions.</p><p>Faster R-CNN: Use convolutional network to propose regions.</p>]]></content:encoded>
      
      <comments>http://yoursite.com/2018/11/27/notes_for_dl/#disqus_thread</comments>
    </item>
    
    <item>
      <title>目标检测模型评估指标:mAP</title>
      <link>http://yoursite.com/2018/11/24/mAP/</link>
      <guid>http://yoursite.com/2018/11/24/mAP/</guid>
      <pubDate>Sat, 24 Nov 2018 02:19:28 GMT</pubDate>
      <description>
      
        
        
          &lt;p&gt;这些天看论文很多处用到mAP，一直只是知道这是一个评估指标，但具体怎么得来的不太清楚，现在有空学习一下~&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Mean Average Precision(mAP)用于评估目标检测模型的物体分类和定位性能，由于该类问题中每一个图片都可能包含许多不同类别的
        
      
      </description>
      
      <content:encoded><![CDATA[<p>这些天看论文很多处用到mAP，一直只是知道这是一个评估指标，但具体怎么得来的不太清楚，现在有空学习一下~</p><hr><p>Mean Average Precision(mAP)用于评估目标检测模型的物体分类和定位性能，由于该类问题中每一个图片都可能包含许多不同类别的物体，故图像分类问题的标准指标<em>precision</em>并不适用。</p><h2 id="Ground-Truth"><a href="#Ground-Truth" class="headerlink" title="Ground Truth"></a>Ground Truth</h2><p>对于目标检测问题，真实标签数据应包括图像中物体的类别以及该图像中每个物体的真实边界框。</p><h3 id="鉴别正确的检测结果并计算precision和recall"><a href="#鉴别正确的检测结果并计算precision和recall" class="headerlink" title="鉴别正确的检测结果并计算precision和recall"></a>鉴别正确的检测结果并计算<em>precision</em>和<em>recall</em></h3><p>首先需得到<em>True Positives</em>、<em>False Positives</em>、<em>True Negatives</em>、<em>False Negatives</em>。</p><p>为了得到TP和FP，需要使用IoU，即预测框与<em>ground truth</em>的交集与并集之比，从而确定一个检测结果是正确的还是错误的。最常用的阈值为0.5，即IoU&gt;0.5，认为是TP，否则FP（当然在一些数据集的评估指标中常采用不同的阈值）。</p><p>计算<em>Recall</em>需要得到负样本的数量，但由于图片中并未预测到物体的每个部分都视作<em>Negative</em>，故很难得到TN。但可以只计算FN，即模型漏检的物体数。</p><p>另一个需要考虑的是模型所给出的各个检测结果的置信度。通过改变置信度阈值可以改变一个预测框是属于<em>Positive</em>还是<em>Negative</em>。阈值以上的所有预测（Box + Class）均被认为是<em>Positve</em>。</p><p>对于每一张图片，<em>ground truth</em>数据会给出图片中各个类别的实际物体数，计算每个<em>Positive</em>预测框与<em>ground truth</em>的IoU，并取值最大的预测框。然后根据IoU阈值，可以得到各个类别的TP和FP。由此，可计算出各个类别的<em>precision</em>。</p><p>由TP，可以计算得到漏检的物体数，即FN。由此，可以计算出各个类别的<em>recall</em>。</p><hr><h2 id="计算mAP"><a href="#计算mAP" class="headerlink" title="计算mAP"></a>计算mAP</h2><p>mAP有许多不同的定义，在这里以PASCAL VOC竞赛的评估指标为例。</p><p>在这里，可以注意到，至少有两个变量会影响<em>precision</em>和<em>recall</em>，即IoU和置信度阈值。在PASCAL VOC竞赛中，IoU采用0.5。但置信度在不同模型中差异较大，会导致precision-recall曲线变化。于是，他们提出计算AP的一种方法。</p><p>首先对模型预测结果进行排序，按<strong>预测值置信度</strong>降序，给定一个<em>rank</em>，<em>recall</em>和<em>precision</em>仅在高于该<em>rank</em>值的预测结果中进行计算。改变<em>rank</em>会导致<em>recall</em>值的变化，共选择11个不同的<em>recall</em>值，分别为0,0.1,0.2,…,1.0，可认为选择了11个<em>rank</em>。由于按照置信度排序，实际就等价于选择了11个置信度阈值。</p><p><img src="https://upload-images.jianshu.io/upload_images/14184962-68a74391a33dcc79.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/600" alt=""></p><p>另外，在计算<em>precision</em>时，采用了插值的方法，对于某个<em>recall</em>值r，<em>precision</em>值取所有recall&gt;=r中的<strong>最大值</strong>，以保证PR曲线是单调递减的，减少由于样本排序中小的变化引起的PR曲线的抖动。AP就定义为在这11个<em>recall</em>下的<em>precision</em>的平均值，表征整个precision-recall曲线，也就是曲线下的面积。<em>average</em>是对<em>recall</em>取平均，<em>mean</em>是对所有类别取平均（每一个类别当做一次二分类任务）。</p><p>对于各个类别，用上述方法计算AP，求所有类别平均即得到mAP。<br>在COCO数据集中，采用的是更严格的计算方式，计算了不同IoU阈值和物体大小下的AP(详情见<a href="http://cocodataset.org/#detection-eval" target="_blank" rel="noopener">COCO Detection Evaluation</a>)。</p><h2 id="举个例子加深自己的理解："><a href="#举个例子加深自己的理解：" class="headerlink" title="举个例子加深自己的理解："></a>举个例子加深自己的理解：</h2><p>一个二分类问题，每个类分别五个样本，如果分类器性能足够好的话，那么根据预测的置信度降序排序，ranking结果应该是+1, +1, +1, +1, +1, -1, -1, -1, -1, -1。然而实际情况中，分类器预测的label和score都不可能如此完美；按score降序，加入根据给定的<em>rank</em>值，选择了前四个score(这个置信分数可能由softmax、SVM等方式计算得到)，认为这四个是正样本，由此来计算<em>recall</em>和<em>precision</em>。而实际上这四个样本中只有两个是正样本，那么此时的<em>recall</em>=2(选择中包含的正样本数)/5(总共的正样本数)=0.4，<em>precision</em>=2(选择中的正样本数)/4(认为是正样本的个数)=0.5。</p><p>可以看出，<em>recall</em>和<em>precision</em>均和<em>rank</em>值有关，也就是由此选择前k个样本，关于k的函数。那么根据<em>rank</em>取值的不同，这里总共可以计算10对precision-recall值，recall依次为为1/，将它们画出来，得到的就是PR曲线。</p><p>观察可以得到PR曲线的一个趋势就是，<em>recall</em>值越高，<em>precision</em>就越低。假如我选择所有样本来计算，那么当然包括了所有的正样本，此时的<em>recall</em>=1(此处认为ground truth所有框均被检测出来。这里有个问题需要思考下，在实际的目标检测问题中，是根据检测出的bbox来计算其与ground truth的IoU以判断bbox是否为正确检测，这样判定出的正确样本数为bbox的数量，往往不等于ground truth的数量；不过bbox的数量是需要经过NMS处理的，处理后基本与检测的目标数差不太多，)，<em>precision</em>就等于所有样本中正样本占比，当负样本占比很大时，那么<em>precision</em>就很小了。</p><hr><h2 id="内容参考："><a href="#内容参考：" class="headerlink" title="内容参考："></a>内容参考：</h2><p>博客：<a href="http://tarangshah.com/blog/2018-01-27/what-is-map-understanding-the-statistic-of-choice-for-comparing-object-detection-models/" target="_blank" rel="noopener">Measuring Object Detection models - mAP - What is Mean Average Precision?</a><br>代码：<a href="https://github.com/facebookresearch/Detectron/blob/05d04d3a024f0991339de45872d02f2f50669b3d/lib/datasets/voc_eval.py" target="_blank" rel="noopener">VOC数据集的mAP实现</a>、<a href="https://github.com/cocodataset/cocoapi/blob/master/PythonAPI/pycocotools/cocoeval.py" target="_blank" rel="noopener">COCO数据集mAP计算API</a><br>论文：<a href="https://arxiv.org/pdf/1607.03476.pdf" target="_blank" rel="noopener">End-to-end training of object class detectors for mean average precision</a></p>]]></content:encoded>
      
      <comments>http://yoursite.com/2018/11/24/mAP/#disqus_thread</comments>
    </item>
    
    <item>
      <title>感受野（Receptive Field）</title>
      <link>http://yoursite.com/2018/11/23/ReceptiveField/</link>
      <guid>http://yoursite.com/2018/11/23/ReceptiveField/</guid>
      <pubDate>Fri, 23 Nov 2018 10:59:52 GMT</pubDate>
      <description>
      
        
        
          &lt;p&gt;之前一直不是很理解感受野是什么，因此认真地看了下相关知识，在这里记录一下~&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&quot;感受野是什么&quot;&gt;&lt;a href=&quot;#感受野是什么&quot; class=&quot;headerlink&quot; title=&quot;感受野是什么&quot;&gt;&lt;/a&gt;感受野是什么&lt;/h2&gt;&lt;p&gt;感受野，
        
      
      </description>
      
      <content:encoded><![CDATA[<p>之前一直不是很理解感受野是什么，因此认真地看了下相关知识，在这里记录一下~</p><hr><h2 id="感受野是什么"><a href="#感受野是什么" class="headerlink" title="感受野是什么"></a>感受野是什么</h2><p>感受野，指的是一个特定的CNN特征在输入空间所受影响的区域，可以用<strong>中心位置</strong>(<em>center location</em>)和<strong>大小</strong>(<em>size</em>)来表征；用来表示网络内部不同神经元对原图像的感受范围大小，也就是CNN每一层输出的特征图（<em>feature map</em>）上的像素点在原始图像上映射的区域大小。</p><p>由于网络结构中普遍使用卷积层和池化层，层与层之间均通过<em>sliding filter</em>进行局部相连，所以每一个神经元都无法对原始图像的所有信息进行感知。而感受野越大，表示该神经元所能接触到的原始图像范围越大，也就意味着更可能蕴含全局、语义层的深层特征；感受野越小则表示其所包含的特征更趋向于局部、浅层的细节特征。</p><p><strong>感受野的大小可以用来判断网络每一层的抽象层次。</strong></p><hr><p>输入层每个单元的感受野是1，层次越深，感受野越大，是由<em>kernel size</em>和<em>stride size</em>共同决定的。而对于深度CNN，我们无法直接追踪到感受野信息。</p><h2 id="感受野中心位置及大小计算"><a href="#感受野中心位置及大小计算" class="headerlink" title="感受野中心位置及大小计算"></a>感受野中心位置及大小计算</h2><p>具体公式推导及代码参考：<a href="https://medium.com/mlreview/a-guide-to-receptive-field-arithmetic-for-convolutional-neural-networks-e0f514068807" target="_blank" rel="noopener">A guide to receptive field arithmetic for Convolutional Neural Networks</a></p><hr><p>对于一个CNN特征来说，感受野中的每个像素值并不是同等重要。一个像素点越接近感受野中心，它对输出特征的计算所起作用越大，这意味着某一个特征不仅仅是受限在输入图片某个特定的区域，而且呈指数级聚焦在区域的中心。</p>]]></content:encoded>
      
      <comments>http://yoursite.com/2018/11/23/ReceptiveField/#disqus_thread</comments>
    </item>
    
    <item>
      <title>目标检测学习之路</title>
      <link>http://yoursite.com/2018/11/20/object_dection/</link>
      <guid>http://yoursite.com/2018/11/20/object_dection/</guid>
      <pubDate>Tue, 20 Nov 2018 03:41:41 GMT</pubDate>
      <description>
      
        
        
          &lt;p&gt;未写完…&lt;/p&gt;
&lt;p&gt;最近为了做毕设，重新开始看目标检测相关的一系列论文，在这里记录一下自己的学习过程，也方便以后复习。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&quot;R-CNN-2013-11&quot;&gt;&lt;a href=&quot;#R-CNN-2013-11&quot; class=&quot;headerlink&quot;
        
      
      </description>
      
      <content:encoded><![CDATA[<p>未写完…</p><p>最近为了做毕设，重新开始看目标检测相关的一系列论文，在这里记录一下自己的学习过程，也方便以后复习。</p><hr><h2 id="R-CNN-2013-11"><a href="#R-CNN-2013-11" class="headerlink" title="R-CNN / 2013.11"></a>R-CNN / 2013.11</h2><p>论文：<a href="https://arxiv.org/abs/1311.2524" target="_blank" rel="noopener">Rich feature hierarchies for accurate object detection and semantic segmentation</a></p><p><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Girshick%2C+R" target="_blank" rel="noopener">Ross Girshick</a>等人发表的这篇文章，首次将卷积神经网络用于「目标检测」，从这以后才有越来越多人将深度学习用于该问题，所以该论文意义深远。<br><img src="https://upload-images.jianshu.io/upload_images/14184962-b09b84954606caab.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/480" alt=""></p><p>主要解决了传统方法的两个问题：</p><p><strong>1. 速度</strong><br>R-CNN使用启发式方法（Selective Search) ，先生成候选区域再检测，降低信息的冗余度，从而提高了检测速度。</p><p><strong>2. 特征提取</strong><br>传统的手工提取，特征仅限于低层次的颜色、纹理等，故效果差。</p><hr><h2 id="SPP-Net-2014-6"><a href="#SPP-Net-2014-6" class="headerlink" title="SPP-Net / 2014.6"></a>SPP-Net / 2014.6</h2><p>论文：<a href="https://arxiv.org/abs/1406.4729" target="_blank" rel="noopener">Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition</a></p><p>在R-CNN提出半年后，<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=He%2C+K" target="_blank" rel="noopener">何恺明</a>等人提出了SPP-Net。</p><p>R-CNN中存在两个比较大的问题，SPP-Net就解决了这两个问题：<br><strong>1. 算力冗余</strong><br>先生成候选区域，再对区域进行卷积，这使得候选区域会有一定程度上的重叠，对相同的区域进行重复的卷积，而每个区域进行新的卷积还需要新的存储空间。</p><p>SPP-Net对此进行了优化，将先生成候选区域再卷积，改为先卷积再生成候选区域。这样，不仅减少了存储量而且加快了训练速度。</p><p><strong>2. 图片的裁剪与缩放</strong><br>在SPP-Net之前，所有的神经网络都是需要输入固定尺寸的照片，比如224*224（ImageNet）、32*32(LenNet)等。这样当我们希望检测各种大小的图片时，需要经过crop，或者warp等一系列操作，这都在一定程度上导致图片信息的丢失和变形，限制了识别精确度。而且，从生理学角度出发，人眼看到一个图片时，大脑会首先认为这是一个整体，而不会进行crop和warp，所以更有可能的是，我们的大脑通过搜集一些浅层的信息，在更深层才识别出这些任意形状的目标。<br><img src="https://upload-images.jianshu.io/upload_images/14184962-21a69ced5d0e9931.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/480" alt=""></p><p><strong>为什么需要固定输入图片的大小？</strong></p><p>卷积层的参数和输入大小无关，它仅是一个卷积核在图像上滑动，对不同大小的图片卷积得到不同大小的特征图（feature map)；但是<strong>全连接层（FC Layer）</strong>情况则不一样。由于全连接层把输入的所有像素点连接起来，需要指定输入层神经元个数和输出层神经元个数，所以必须规定输入的特征图大小，以指定参数个数。</p><p><strong>SPP-Net解决方法</strong></p><p> SPP-Net在最后一个卷积层后，接入了<strong>金字塔池化（spatial pyramid pooling)层</strong>，使用这种方式，可以让网络输入任意的图片，而且还会生成固定大小的输出。</p><p><strong>什么是金字塔池化？</strong><br><img src="https://upload-images.jianshu.io/upload_images/14184962-6b7b39eab6305a67.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/480" alt=""><br>从图中分析，黑色图片代表Conv5之后的feature map，其大小任意。接着以不同大小的块来提取特征，分别是4*4，2*2，1*1，将这三张网格放到下面这张特征图上，就可以得到16+4+1=21种不同的块(Spatial bins)，我们从这21个块中，每个块提取出一个特征，这样刚好就是我们要提取的21维特征向量。这种以不同的大小格子的组合方式来池化的过程就是空间金字塔池化（SPP）。如果要进行空间金字塔最大池化，就是从这21个图片块中，分别计算每个块的最大值，从而得到一个21维特征的输出单元。共用256个滤波器，最终输出向量大小固定为<strong>bins*filters</strong>，即（16+4+1)*256，作为全连接层的输入。</p><p>总的来说，金字塔池化意义在于<strong>多尺度特征提取出固定大小的特征向量</strong>。</p><hr><h2 id="Fast-R-CNN-2015-4"><a href="#Fast-R-CNN-2015-4" class="headerlink" title="Fast R-CNN / 2015.4"></a>Fast R-CNN / 2015.4</h2><p>论文：<a href="http://link.zhihu.com/?target=https%3A//arxiv.org/abs/1504.08083" target="_blank" rel="noopener">Fast R-CNN</a></p><p><a href="https://arxiv.org/abs/1504.08083" target="_blank" rel="noopener">Ross Girshick</a>在R-CNN之后，再次提出了Fast R-CNN，该论文中引用了SPP-Net的工作，相对于之前的R-CNN所做的最大优化就是<strong>快</strong>。</p><p><strong>将原来的串行结构改成并行结构</strong></p>]]></content:encoded>
      
      <comments>http://yoursite.com/2018/11/20/object_dection/#disqus_thread</comments>
    </item>
    
  </channel>
</rss>
