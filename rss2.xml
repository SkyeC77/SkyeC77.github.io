<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"
  xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Notes</title>
    <link>http://yoursite.com/</link>
    <atom:link href="/rss2.xml" rel="self" type="application/rss+xml"/>
    
    <description></description>
    <pubDate>Fri, 06 Dec 2019 10:49:36 GMT</pubDate>
    <generator>http://hexo.io/</generator>
    
    <item>
      <title>The Love Letter --200天纪念</title>
      <link>http://yoursite.com/2019/12/06/loveletter1/</link>
      <guid>http://yoursite.com/2019/12/06/loveletter1/</guid>
      <pubDate>Fri, 06 Dec 2019 10:42:21 GMT</pubDate>
      <description>
      
        
        
          &lt;p&gt;二狗子写给我的第一封情书~&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;琪琦：&lt;/p&gt;
&lt;p&gt;展信悦。&lt;/p&gt;
&lt;p&gt;来井冈山的路上火车经过山区，恰好是晚上，天是暗的，山是暗的，树也是暗的，在一片寂静与黑暗里，有些许人家点亮着灯光。琪琦，我这时想起你。我以前以为自己是个能忍受寂寞的人，后来和你
        
      
      </description>
      
      <content:encoded><![CDATA[<p>二狗子写给我的第一封情书~</p><hr><p>琪琦：</p><p>展信悦。</p><p>来井冈山的路上火车经过山区，恰好是晚上，天是暗的，山是暗的，树也是暗的，在一片寂静与黑暗里，有些许人家点亮着灯光。琪琦，我这时想起你。我以前以为自己是个能忍受寂寞的人，后来和你在一起以后，变成了一个黏人的小孩子。对我而言，你就是这一盏灯火，滚滚红尘里我回头知道还有一个人在默默等我回家。</p><p>昨天报名之后的大把闲暇时光，我也与你说过，在学院里面转了转，有凉亭、秋千和对坐而谈的老人。荡着秋千时我曾想着若是你在该多好，我推着你慢悠悠地晃荡，你和我说着耳鬓厮磨的情话。我就这样一边生活一边想你，我快乐时希望与你一同快乐，我无聊时想着你在我便不会无聊。我盼望下一个假期快些到来，可我还得再数三十多个日子，三十多个日日夜夜里我该被相思折磨得有多煎熬。</p><p>为了想与你说话，我在睡觉前常常回想我们的事情，渴望在沉沉睡梦中再见你一眼。琪琦，你看，我睡的地方多么舒适，这里的空气多么清新，外面的风也温柔，它绕着我们用力说，快亲吻呀，快抱住她。风确实在沙沙地吹，但我又去同谁说呢。我好想你，连梦里都低声呼唤你的名字。</p><p>前些天，我看到了一句话，很想老了以后能对你说。</p><p>我这一生都是坚定的唯物主义者，唯独对你，我渴望有来生。</p><p>我渴望和你在一起的每一生。</p><p>吻你。</p><p>想你的男朋友</p><p>2019.08.30在一起200天纪念日</p><p>于井冈山干部学院</p><hr>]]></content:encoded>
      
      <comments>http://yoursite.com/2019/12/06/loveletter1/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Caffe--&quot;deploy.prototxt&quot; and &quot;train_val.prototxt&quot;</title>
      <link>http://yoursite.com/2018/12/15/deploy/</link>
      <guid>http://yoursite.com/2018/12/15/deploy/</guid>
      <pubDate>Sat, 15 Dec 2018 01:55:12 GMT</pubDate>
      <description>
      
        
        
          &lt;p&gt;最近学习了一下Caffe，并且用迁移学习完成了一个简单的猫狗分类问题。在学习的过程中，踩了很多坑，这里主要记录一下关于几个prototxt配置文件的问题。&lt;/p&gt;
&lt;h2 id=&quot;prototxt文件&quot;&gt;&lt;a href=&quot;#prototxt文件&quot; class=&quot;header
        
      
      </description>
      
      <content:encoded><![CDATA[<p>最近学习了一下Caffe，并且用迁移学习完成了一个简单的猫狗分类问题。在学习的过程中，踩了很多坑，这里主要记录一下关于几个prototxt配置文件的问题。</p><h2 id="prototxt文件"><a href="#prototxt文件" class="headerlink" title="prototxt文件"></a>prototxt文件</h2><ul><li>train_val.prototxt：网络配置文件，用于训练</li><li>deploy.prototxt：网络配置文件，用于inference，测试</li><li>solver.prototxt：参数文件，用于训练，设置了学习率、优化方法等，slover函数生成</li></ul><h2 id="fine-tune"><a href="#fine-tune" class="headerlink" title="fine-tune"></a>fine-tune</h2><p>为了将在ImageNet上训练好的模型用于自己的数据集分类问题，我们需要对网络进行迁移学习或者微调。</p><p>下面以GoogleNet为例对几点需要注意的地方进行说明：</p><ol><li>先准备好训练数据与测试数据。在train_val.prototxt文件中，如果输入的数据可以是图片格式，也可以是存储在数据库中的lmdb格式，可以加快读取速度。两种情况下数据层的配置有区别，如下：</li></ol><p><strong>Image格式</strong>：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">layer &#123;</span><br><span class="line">  name: &quot;data&quot;</span><br><span class="line">  type: &quot;ImageData&quot;</span><br><span class="line">  top: &quot;data&quot;</span><br><span class="line">  top: &quot;label&quot;</span><br><span class="line">  include &#123;</span><br><span class="line">    phase: TRAIN</span><br><span class="line">  &#125;</span><br><span class="line">  image_data_param &#123;</span><br><span class="line">    source: &quot;train.txt&quot;</span><br><span class="line">    new_height: 224</span><br><span class="line">    new_width: 224</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">layer &#123;</span><br><span class="line">  name: &quot;data&quot;</span><br><span class="line">  type: &quot;ImageData&quot;</span><br><span class="line">  top: &quot;data&quot;</span><br><span class="line">  top: &quot;label&quot;</span><br><span class="line">  include &#123;</span><br><span class="line">    phase: TEST</span><br><span class="line">  &#125;</span><br><span class="line">  image_data_param &#123;</span><br><span class="line">    source: &quot;val.txt&quot;</span><br><span class="line">    batch_size: 50</span><br><span class="line">    new_height: 224</span><br><span class="line">    new_width: 224</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>其中train.txt和val.txt分别是保存了训练集和测试集的路径的文件，直接以图片格式读入。</p><p><strong>lmdb格式</strong>：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">layer &#123;</span><br><span class="line">  name: &quot;data&quot;</span><br><span class="line">  type: &quot;Data&quot;</span><br><span class="line">  top: &quot;data&quot;</span><br><span class="line">  top: &quot;label&quot;</span><br><span class="line">  include &#123;</span><br><span class="line">    phase: TRAIN</span><br><span class="line">  &#125;</span><br><span class="line">  transform_param &#123;</span><br><span class="line">    mirror: true</span><br><span class="line">    crop_size: 224</span><br><span class="line">    mean_value: 104</span><br><span class="line">    mean_value: 117</span><br><span class="line">    mean_value: 123</span><br><span class="line">  &#125;</span><br><span class="line">  data_param &#123;</span><br><span class="line">    source: &quot;examples/imagenet/ilsvrc12_train_lmdb&quot;</span><br><span class="line">    batch_size: 32</span><br><span class="line">    backend: LMDB</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">layer &#123;</span><br><span class="line">  name: &quot;data&quot;</span><br><span class="line">  type: &quot;Data&quot;</span><br><span class="line">  top: &quot;data&quot;</span><br><span class="line">  top: &quot;label&quot;</span><br><span class="line">  include &#123;</span><br><span class="line">    phase: TEST</span><br><span class="line">  &#125;</span><br><span class="line">  transform_param &#123;</span><br><span class="line">    mirror: false</span><br><span class="line">    crop_size: 224</span><br><span class="line">    mean_value: 104</span><br><span class="line">    mean_value: 117</span><br><span class="line">    mean_value: 123</span><br><span class="line">  &#125;</span><br><span class="line">  data_param &#123;</span><br><span class="line">    source: &quot;examples/imagenet/ilsvrc12_val_lmdb&quot;</span><br><span class="line">    batch_size: 50</span><br><span class="line">    backend: LMDB</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>这个需要自己利用train.txt和val.txt文件从硬盘读取图像数据保存在LMDB数据库，该数据格式采用内存-映射，故I/O效率高。</p><ol start="2"><li><p>需要改变最后一个fc层的名称（type:”InnerProduct”），并将输出维度改为我们的类别数，也就相当于重头开始学习一个新的层，这样在训练时，模型就会去fit我们的训练数据。</p></li><li><p>当自己的数据集够大时，可以在训练过程中对所有网络权重进行更新；但是本身任务的数据集较小时，为了避免过拟合，需要冻结一部分网络，一般是选择固定前面层的参数，不更新权重，只更新最后几层（迁移学习时冻结前面所有层，只对最后一个fc层进行权重更新，训练分类器）。这是因为网络的前几层得到的特征是比较基础local的特征，基本适用于全部任务（边缘，角点特征），而网络层数越高的层，和全局信息联系越紧密，local的信息是多数情况共享的，而global的信息和原图紧密相关。</p></li></ol><p>具体的不更新权重的方法在caffe中就是将该层param中的学习率设为0：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">layer &#123;</span><br><span class="line">  name: &quot;conv1/7x7_s2&quot;</span><br><span class="line">  type: &quot;Convolution&quot;</span><br><span class="line">  bottom: &quot;data&quot;</span><br><span class="line">  top: &quot;conv1/7x7_s2&quot;</span><br><span class="line">  param &#123;</span><br><span class="line">    lr_mult: 0</span><br><span class="line">    decay_mult: 1</span><br><span class="line">  &#125;</span><br><span class="line">  param &#123;</span><br><span class="line">    lr_mult: 0</span><br><span class="line">    decay_mult: 0</span><br><span class="line">  &#125;</span><br><span class="line">  convolution_param &#123;</span><br><span class="line">    num_output: 64</span><br><span class="line">    pad: 3</span><br><span class="line">    kernel_size: 7</span><br><span class="line">    stride: 2</span><br><span class="line">    weight_filler &#123;</span><br><span class="line">      type: &quot;xavier&quot;</span><br><span class="line">    &#125;</span><br><span class="line">    bias_filler &#123;</span><br><span class="line">      type: &quot;constant&quot;</span><br><span class="line">      value: 0.2</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><ol start="4"><li><p>在训练过程中一般将偏置项b的学习率设置为权重矩阵w的两倍，最终的学习率等于train_val.prototxt中的lr_mult*base_lr（slover.prototxt中设置）。在fine-tune时，base_lr要设置得很小，一般在1e-5数量级，而最后一层的lr_mult设置得较大，因为该层是重新学习，一般是其它层的10倍。</p></li><li><p>solver参数都需要进行修改，原来的参数适用于training from scratch，学习率、步长、迭代次数都较大，我们都需要改得比较小。</p></li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">#Based on https://github.com/BVLC/caffe/tree/master/models/bvlc_googlenet</span><br><span class="line">net: &quot;train_val.prototxt&quot;</span><br><span class="line">display: 50</span><br><span class="line">base_lr: 0.0001</span><br><span class="line">lr_policy: &quot;fixed&quot;</span><br><span class="line">max_iter: 15000</span><br><span class="line">momentum: 0.9</span><br><span class="line">weight_decay: 0.0002</span><br><span class="line">snapshot_prefix: &quot;transfer_learning&quot;</span><br><span class="line">solver_mode: CPU</span><br><span class="line">test_iter: 100</span><br><span class="line">test_interval: 1000</span><br></pre></td></tr></table></figure><ol start="6"><li>在做inference时，使用deploy.prototxt文件，其实是train_val.prototxt文件改动了前面的数据层以及最后的计算loss的部分。这时数据层很简单：</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">layer &#123;</span><br><span class="line">  name: &quot;data&quot;</span><br><span class="line">  type: &quot;Input&quot;</span><br><span class="line">  top: &quot;data&quot;</span><br><span class="line">  input_param &#123; shape: &#123; dim: 1 dim: 3 dim: 224 dim: 224 &#125; &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>而针对内积之后的softmax输出层，训练和测试时也有差别：</p><p>training:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">layer &#123;</span><br><span class="line">  name: &quot;loss3/loss3&quot;</span><br><span class="line">  type: &quot;SoftmaxWithLoss&quot;</span><br><span class="line">  bottom: &quot;loss3/classifier_dc&quot;</span><br><span class="line">  bottom: &quot;label&quot;</span><br><span class="line">  top: &quot;loss3/loss3&quot;</span><br><span class="line">  loss_weight: 1</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>inference:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">layer &#123;</span><br><span class="line">  name: &quot;prob&quot;</span><br><span class="line">  type: &quot;Softmax&quot;</span><br><span class="line">  bottom: &quot;loss3/classifier_dc&quot;</span><br><span class="line">  top: &quot;prob&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>因为测试时不需要计算误差进行bp，也就不需要label，故bottom层只有一个，type也不同。</p>]]></content:encoded>
      
      <comments>http://yoursite.com/2018/12/15/deploy/#disqus_thread</comments>
    </item>
    
    <item>
      <title>正则化与先验分布的关系</title>
      <link>http://yoursite.com/2018/12/14/regularization/</link>
      <guid>http://yoursite.com/2018/12/14/regularization/</guid>
      <pubDate>Fri, 14 Dec 2018 01:20:17 GMT</pubDate>
      <description>
      
        
        
          &lt;p&gt;最近从新开始学习机器学习，在看《统计学习方法》一书，其中提到：&lt;strong&gt;从贝叶斯估计的角度来看，正则化项对应于模型的先验概率。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;然后发现自己无法理解这句话的意思，于是，求助于google，找到了一篇写的还不错的博客：[从贝叶斯的角度来看
        
      
      </description>
      
      <content:encoded><![CDATA[<p>最近从新开始学习机器学习，在看《统计学习方法》一书，其中提到：<strong>从贝叶斯估计的角度来看，正则化项对应于模型的先验概率。</strong></p><p>然后发现自己无法理解这句话的意思，于是，求助于google，找到了一篇写的还不错的博客：[从贝叶斯的角度来看，正则化等价于对模型参数引入先验分布]<a href="https://www.zhihu.com/question/23536142/answer/90135994" target="_blank" rel="noopener">https://www.zhihu.com/question/23536142/answer/90135994</a></p><p>但是看上面那篇博客时，我发现，这篇文中有也很多我不懂的地方，首先我需要知道什么是贝叶斯回归…于是，继续google，然后，找到了另一篇通俗易懂的博客，分析了我们常用的频率线性回归和贝叶斯线性回归：[Introduction to Bayesian Linear Regression]<a href="https://towardsdatascience.com/introduction-to-bayesian-linear-regression-e66e60791ea7" target="_blank" rel="noopener">https://towardsdatascience.com/introduction-to-bayesian-linear-regression-e66e60791ea7</a></p><hr><p>总之，感受到自己的理论基础太差了，要学的东西很多；尤其是数学这一块，一直是我最头痛的地方，但是，还是要认真学啊…</p>]]></content:encoded>
      
      <comments>http://yoursite.com/2018/12/14/regularization/#disqus_thread</comments>
    </item>
    
    <item>
      <title>yolov3训练可视化</title>
      <link>http://yoursite.com/2018/12/05/visuialization/</link>
      <guid>http://yoursite.com/2018/12/05/visuialization/</guid>
      <pubDate>Wed, 05 Dec 2018 02:24:38 GMT</pubDate>
      <description>
      
        
        
          &lt;p&gt;这里记录一下训练yolov3时将输出记录并可视化的整个过程~&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&quot;将Terminal输出到文本文件&quot;&gt;&lt;a href=&quot;#将Terminal输出到文本文件&quot; class=&quot;headerlink&quot; title=&quot;将Terminal输出到文本文件&quot;
        
      
      </description>
      
      <content:encoded><![CDATA[<p>这里记录一下训练yolov3时将输出记录并可视化的整个过程~</p><hr><h2 id="将Terminal输出到文本文件"><a href="#将Terminal输出到文本文件" class="headerlink" title="将Terminal输出到文本文件"></a>将Terminal输出到文本文件</h2><p><img src="https://upload-images.jianshu.io/upload_images/14184962-15b455a8e41364f9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>说明：”&lt;”将输出转向，将命令行输出写入文本文件，并会清空原文件中的所有内容；而用”&lt;&lt;”则会将输出附加到文件后面，原内容保留。</p><p>但是这样命令行就看不到输出结果了。要在输出结果的同时将信息记录在文件中，则需使用命令<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">![](https://upload-images.jianshu.io/upload_images/14184962-0289fc45340a45f7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)</span><br><span class="line"></span><br><span class="line">该命令从标准输入读取内容并将其写到标准输出和文件中。</span><br><span class="line">***</span><br><span class="line">以上是在服务器训练时常用的。而对于一个计算平台，当我需要训练一个网络时，作为普通用户，我是在登录节点用自己的id登录，然后部署自己所需要的环境。当我把环境配置好了，需要跑网络时，就不能直接在该登录节点跑，这样会大量占用别人的资源。这时，需要递交一个作业到计算节点，利用GPU群进行高性能计算。一般是递交一个.sh文件，将所有命令都写在文件中。</span><br><span class="line"></span><br><span class="line">那么这个时候网络在计算节点训练，我在登录节点是没法看到输出结果的，但是我将输出都写入了日志文件。那么这个时候我想查看输出情况，该怎么办？对于一个不断更新的日志文件，如果直接打开对其进行读操作是会破坏其写入过程的，这个时候就需要用到```tail```命令了。</span><br><span class="line"></span><br><span class="line">```tail -f log.879.out</span><br></pre></td></tr></table></figure></p><p>这个命令会输出日志的最后十行，<code>-f</code>使其不断滚动更新，这样，我就可以监视输出结果了。</p><hr><h2 id="日志的可视化"><a href="#日志的可视化" class="headerlink" title="日志的可视化"></a>日志的可视化</h2>]]></content:encoded>
      
      <comments>http://yoursite.com/2018/12/05/visuialization/#disqus_thread</comments>
    </item>
    
    <item>
      <title>train yolov3</title>
      <link>http://yoursite.com/2018/12/03/train_yolov3/</link>
      <guid>http://yoursite.com/2018/12/03/train_yolov3/</guid>
      <pubDate>Mon, 03 Dec 2018 05:17:46 GMT</pubDate>
      <description>
      
        
        
          &lt;p&gt;这些天在复现YOLOV3时，训练过程中遇到了一些问题，在这里记录一下跑网络的过程~&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&quot;训练指令&quot;&gt;&lt;a href=&quot;#训练指令&quot; class=&quot;headerlink&quot; title=&quot;训练指令&quot;&gt;&lt;/a&gt;训练指令&lt;/h2&gt;&lt;p&gt;注意根据情况修改
        
      
      </description>
      
      <content:encoded><![CDATA[<p>这些天在复现YOLOV3时，训练过程中遇到了一些问题，在这里记录一下跑网络的过程~</p><hr><h2 id="训练指令"><a href="#训练指令" class="headerlink" title="训练指令"></a>训练指令</h2><p>注意根据情况修改Makefile文件，并重新make，以及cfg文件夹下yolov3-voc.cfg文件，在darknet目录下执行命令：</p><p>格式：<code>./darknet detector train [data_cfg] [network_cfg] [weight] -gpus [gpu_nums]</code></p><p><strong>CPU上训练</strong>：</p><p><code>./darknet -ngpu detector train cfg/voc.data cfg/yolov3-voc.cfg darknet.conv.74</code></p><p><strong>多GPU训练</strong>：</p><p><code>./darknet detector train cfg/voc.data cfg/yolov3-voc.cfg darknet.conv.74 -gpus 0,1</code></p><p><strong>输出重定向</strong></p><p>如果需要将训练过程可视化，则训练时将输出重定向得到训练日志文件；</p><p><code>./darknet detector train cfg/voc.data cfg/yolov3-voc.cfg darknet.conv.74 -gpus 0,1 2&gt;1 | tee train_yolov3.log</code></p><p><strong>从检查点开始恢复训练</strong></p><p><code>./darknet detector train cfg/voc.data cfg/yolov3-voc.cfg backup/yolov3.backup -gpus 0,1</code></p><hr><h2 id="输出解释"><a href="#输出解释" class="headerlink" title="输出解释"></a>输出解释</h2><h3 id="forward-yolo-layer"><a href="#forward-yolo-layer" class="headerlink" title="forward_yolo_layer()"></a><em>forward_yolo_layer()</em></h3><p>每个子batch输出一次，详见src文件夹下yolo_layer.c文件的<em>forward_yolo_layer</em>函数。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">printf(&quot;Region %d Avg IOU: %f, Class: %f, Obj: %f, No Obj: %f, .5R: %f, .75R: %f,  count: %d\n&quot;, net.index, avg_iou/count, avg_cat/class_count, avg_obj/count, avg_anyobj/(l.w*l.h*l.n*l.batch), recall/count, recall75/count, count);</span><br></pre></td></tr></table></figure><ul><li><p><code>Region [xx]</code>: cfg文件中的layer的索引；</p></li><li><p><code>Avg IOU</code>: 当前迭代过程中，预测的bbox和ground truth的平均交并比，理想值为1；</p></li><li><p><code>Class</code>: 标注物体的分类准确率，理想值为1；</p></li><li><p><code>Obj</code>: 期望该值越接近1越好；</p></li><li><p><code>No Obj</code>: 期望该值越小越好；</p></li><li><p><code>.5R</code>: IoU=0.5为阈值时，当前子batch中recall值；</p></li><li><p><code>.75R</code>: IoU=0.75为阈值时，当前子batch中recall值；</p></li><li><p><code>count</code>: 当前subdivision的子batch中所有图片中包含的正样本数。<code>count = 0</code>表示该批次该尺度上没有检测到对象，会导致<code>Avg IOU</code>、<code>Class</code>、<code>obj</code>、<code>.5R</code>、<code>.75R</code>值均为<code>-nan</code>或<code>nan</code>。这可能是由于在许多小物体上训练导致的，所以在显存允许的情况下，可适当增大<code>batch</code>的大小，可一定程度减少nan的出现。</p></li></ul><h3 id="train-detector"><a href="#train-detector" class="headerlink" title="train_detector()"></a><em>train_detector()</em></h3><p>每个batch输出一次loss，详见examples文件夹下detect.c文件中<em>train_detector</em>函数:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">printf(&quot;%ld: %f, %f avg, %f rate, %lf seconds, %d images\n&quot;, get_current_batch(net), loss, avg_loss, get_current_rate(net), what_time_is_it_now()-time, i*imgs);</span><br></pre></td></tr></table></figure><hr><h2 id="问题及解决"><a href="#问题及解决" class="headerlink" title="问题及解决"></a>问题及解决</h2><h3 id="CUDA-out-of-memory"><a href="#CUDA-out-of-memory" class="headerlink" title="CUDA: out of memory"></a>CUDA: out of memory</h3><p>显存不够引起的，修改yolov3-voc.cfg文件。</p><ul><li><p>修改<code>[net]</code>层参数，增大<code>subdivisions</code>或调小<code>batch</code>，因为训练时实际每次前向的图片数量 = batch/subdivisions。</p></li><li><p>关闭多尺度训练。将所有<code>[yolo]</code>层中<code>random</code>参数修改为<code>random = 0</code>。</p></li></ul><hr><h2 id="参数调整"><a href="#参数调整" class="headerlink" title="参数调整"></a>参数调整</h2><ol><li><p>Batch_Size：想要模型快速收敛，最好是增大Batch_Size，但可能导致陷入局部最小的情况；减小Batch_Size可能会有更好的效果，但是如果类别数较多，Batch_Size又太小，会导致loss函数震荡不收敛。一般在调试时，根据GPU显存，设置为最大（一般要求为8的倍数），选部分数据跑几个Batch看看loss是否在变小，再选择合适的值。yolov3-voc.cfg文件中对应参数为<code>batch</code>。</p></li><li><p>如果内存不够大，增大<code>subdivision</code>值，将batch分割，每个子batch大小为 batch/subdivisions。</p></li><li><p>为了应对小目标检测，网络输入大一些更好,即增大<code>weight</code>和<code>height</code>。</p></li><li><p><code>random = 1</code>会使用多尺度训练，随机使用不同尺寸的图片进行训练；如果为0，则每次训练图片大小与输入尺寸一致。</p></li></ol>]]></content:encoded>
      
      <comments>http://yoursite.com/2018/12/03/train_yolov3/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Linux下监视NVIDIA的GPU使用情况</title>
      <link>http://yoursite.com/2018/12/02/watch_gpu/</link>
      <guid>http://yoursite.com/2018/12/02/watch_gpu/</guid>
      <pubDate>Sun, 02 Dec 2018 10:20:37 GMT</pubDate>
      <description>
      
        
        
          &lt;p&gt;最近在服务器上跑网络，发现对命令行还是很不熟悉，每次都记不住要靠google，有很多东西要学。还是趁这个机会边学边整理吧。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&quot;显示当前GPU使用情况&quot;&gt;&lt;a href=&quot;#显示当前GPU使用情况&quot; class=&quot;headerlink&quot; tit
        
      
      </description>
      
      <content:encoded><![CDATA[<p>最近在服务器上跑网络，发现对命令行还是很不熟悉，每次都记不住要靠google，有很多东西要学。还是趁这个机会边学边整理吧。</p><hr><h2 id="显示当前GPU使用情况"><a href="#显示当前GPU使用情况" class="headerlink" title="显示当前GPU使用情况"></a>显示当前GPU使用情况</h2><p>在命令行下，Nvidia自带了工具，会显示显存使用情况。命令如下：</p><p><code>$ nvidia-smi</code></p><p>输出如图：</p><p><img src="https://upload-images.jianshu.io/upload_images/14184962-f2a374580f44f847.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><h2 id="nvdia-smi表含义"><a href="#nvdia-smi表含义" class="headerlink" title="nvdia-smi表含义"></a>nvdia-smi表含义</h2><p>表中第一行是显卡版本信息，第二行是标题栏，对应含义如下；</p><p><strong>GPU</strong>：显卡编号<br><strong>Fan</strong>：风扇转速，范围在0~100%<br><strong>Name</strong>：显卡名，这里是四张Geforce GTX 1080Ti<br><strong>Temp</strong>：显卡温度，单位摄氏度<br><strong>Perf</strong>：显卡性能，P0~P12，P0表示最大性能<br><strong>Persistence-M</strong>：持续模式的状态，该模式耗能大，但是在新的GPU启动时，花费的时间更少；这里是off状态<br><strong>Pwr：Usage/Cap</strong>：能耗，当前能耗/最大能耗,单位瓦</p><p><strong>Bus-Id</strong>：GPU总线的相关信息，domain:bus:device.function<br><strong>Disp.A</strong>：GPU的显示是否初始化，display activate<br><strong>Memory-Usage</strong>：显存使用情况，当前占用显存/总显存</p><p><strong>Uncorr. ECC</strong>：关于ECC的信息<br><strong>Volatile GPU-Util</strong>：浮动的GPU利用率<br><strong>Compute M.</strong>：计算模式，这里是默认模式</p><p>表格下半部分显示的是每个显卡上相应进程的显存占用情况。</p><hr><p><code>$ nvidia-smi -h</code></p><p>通过该命令可获得mvidia-smi系统管理界面所有相关信息。</p><hr><h2 id="周期性更新GPU状态"><a href="#周期性更新GPU状态" class="headerlink" title="周期性更新GPU状态"></a>周期性更新GPU状态</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ whatis watch</span><br><span class="line">watch(1)        - execute a program periodically, showing output fullscreen</span><br></pre></td></tr></table></figure><p><code>$ watch [options] nvidia-smi</code></p><p>options为可选参数，通常用<code>-n</code>，后接数字表示多少秒执行一次命令。</p><p><code>$ watch -n 20 nvidia-smi</code></p><p>会进入监视界面，并且每20秒会刷新一次显卡状态，退出监视模式只要按<code>Ctrl+z</code>。</p>]]></content:encoded>
      
      <comments>http://yoursite.com/2018/12/02/watch_gpu/#disqus_thread</comments>
    </item>
    
    <item>
      <title>a virtualenv on linux</title>
      <link>http://yoursite.com/2018/12/01/virtualenv/</link>
      <guid>http://yoursite.com/2018/12/01/virtualenv/</guid>
      <pubDate>Sat, 01 Dec 2018 06:43:22 GMT</pubDate>
      <description>
      
        
        
          &lt;p&gt;前些天在实验室的服务器上部署环境跑网络。然后，自己总是忘记怎么进入自己搭建的虚拟环境，不得已在这里记录一下…&lt;/p&gt;
&lt;p&gt;在自己的笔记本上安装了Anaconda，不过服务器上没有conda，只安装了python-virtualenv，不过效果也差不多。&lt;/p&gt;
&lt;p&gt;例如
        
      
      </description>
      
      <content:encoded><![CDATA[<p>前些天在实验室的服务器上部署环境跑网络。然后，自己总是忘记怎么进入自己搭建的虚拟环境，不得已在这里记录一下…</p><p>在自己的笔记本上安装了Anaconda，不过服务器上没有conda，只安装了python-virtualenv，不过效果也差不多。</p><p>例如：</p><p><strong>创建名为’py3’的虚拟环境</strong></p><p><code>virtualenv py3</code></p><p>默认情况下，虚拟环境会依赖系统环境中的site packages，如果不需要系统环境中的这些第三方包，则需要在上述命令后加上参数 <code>--no-site-packages</code> 来创建虚拟环境。</p><p><strong>启动虚拟环境</strong></p><p><code>cd py3</code><br><code>source ./bin/activate</code></p><p><strong>退出虚拟环境</strong></p><p><code>deactivate</code></p><p>启动虚拟环境后，所有通过pip安装的模块都会被安装在该环境中，不会对系统环境产生影响。要删除该虚拟环境，只需要将相应文件夹删除即可。</p><hr><p>那么在安装了Anaconda时，创建独立的环境命令如下（其中tf为自己取的虚拟环境名称）：</p><p><code>conda create -n tf</code></p><p><strong>进入虚拟环境</strong></p><p><code>source activate tf</code></p><p><strong>安装需要的模块</strong></p><p><code>conda install tensorflow-gpu</code></p><p><strong>列出环境中所有已装的模块</strong></p><p><code>conda list</code></p><p><strong>退出虚拟环境</strong></p><p><code>source deactivate</code></p><p><strong>列出所有的环境(包括base)</strong></p><p><code>conda env list</code></p><p><strong>删除某个虚拟环境</strong></p><p><code>conda remove -n tf -all</code></p>]]></content:encoded>
      
      <comments>http://yoursite.com/2018/12/01/virtualenv/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Caffe &quot;in-place&quot; operation</title>
      <link>http://yoursite.com/2018/11/29/caffe_inplace/</link>
      <guid>http://yoursite.com/2018/11/29/caffe_inplace/</guid>
      <pubDate>Thu, 29 Nov 2018 02:48:25 GMT</pubDate>
      <description>
      
        
        
          &lt;p&gt;在学习Caffe model中关于Layers内容时，发现一个问题，ReLU层的top blob和bottom blob相同，如下：&lt;br&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;
        
      
      </description>
      
      <content:encoded><![CDATA[<p>在学习Caffe model中关于Layers内容时，发现一个问题，ReLU层的top blob和bottom blob相同，如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">layer &#123;</span><br><span class="line">  name: &quot;conv2/relu_3x3_reduce&quot;</span><br><span class="line">  type: &quot;ReLU&quot;</span><br><span class="line">  bottom: &quot;conv2/3x3_reduce&quot;</span><br><span class="line">  top: &quot;conv2/3x3_reduce&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>后来查资料发现这是caffe的in-place操作，是为了节省内（显）存，以及省去反复申请和释放内存的时间。</p><p>bottom blob和top blob名称相同，说明是同一个blob，占用的是内存的同一块空间。在这里也就是说，该ReLU操作是对输入blob本身进行操作，再将其输出。</p><p>如果定义的两个<code>layer</code>的top blob名称是一样的，那么这两个<code>layer</code>的bottom blob也一定是该top blob，并且按<code>layer</code>的定义顺序对该bottom blob进行操作。如果layer不是对bottom blob本身进行操作，那么top blob就不能与bottom blob相同，也不允许多个layer的top blob同名。因为假如这样做，后运算的layer会将top blob覆盖成其运算的结果，前面的layer的运算结果就消失了。</p><p>convolution、pooling层由于输入输出的size一般不一致，所以不能支持in-place操作；而ReLU函数是逐个对元素进行计算，不该变size，所以支持。AlexNet、VGGnet等都是在ReLU层使用in-place计算，ResNet中，BatchNorm和Scale也都使用了in-place计算。</p><p>目前已知支持in-place的有：ReLU层、Dropout层、BatchNorm层、Scale层。(除此之外，其他大部分layer貌似在定义时<code>name</code>和<code>top</code>是相同的。)</p>]]></content:encoded>
      
      <comments>http://yoursite.com/2018/11/29/caffe_inplace/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Yolov3 Network Architecture</title>
      <link>http://yoursite.com/2018/11/28/yolov3/</link>
      <guid>http://yoursite.com/2018/11/28/yolov3/</guid>
      <pubDate>Wed, 28 Nov 2018 09:59:59 GMT</pubDate>
      <description>
      
        
        
          &lt;p&gt;贴一张Yolov3网络结构图~&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://upload-images.jianshu.io/upload_images/14184962-7d6aca0fa7d27e52.jpg?imageMogr2/auto-orient/stri
        
      
      </description>
      
      <content:encoded><![CDATA[<p>贴一张Yolov3网络结构图~</p><p><img src="https://upload-images.jianshu.io/upload_images/14184962-7d6aca0fa7d27e52.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>filter = 3*(80+1+4) = 255</p><p>Yolov3 is a fully convolutional network. It has 75 convolutional layers, with skip connections and upsampling layers. No form of pooling is used, and a convolutional layer with stride 2 is used to downsample the feature maps, which helps in preventing loss of low-level features often attributed to pooling.</p><p>详细内容见：<a href="https://www.cyberailab.com/home/a-closer-look-at-yolov3" target="_blank" rel="noopener"><strong>A Closer Look at YOLOv3</strong></a></p>]]></content:encoded>
      
      <comments>http://yoursite.com/2018/11/28/yolov3/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Some notes for deep learning</title>
      <link>http://yoursite.com/2018/11/27/notes_for_dl/</link>
      <guid>http://yoursite.com/2018/11/27/notes_for_dl/</guid>
      <pubDate>Tue, 27 Nov 2018 15:41:55 GMT</pubDate>
      <description>
      
        
        
          &lt;h2 id=&quot;Object-localization&quot;&gt;&lt;a href=&quot;#Object-localization&quot; class=&quot;headerlink&quot; title=&quot;Object localization&quot;&gt;&lt;/a&gt;Object localization&lt;/h2&gt;&lt;p&gt;监督
        
      
      </description>
      
      <content:encoded><![CDATA[<h2 id="Object-localization"><a href="#Object-localization" class="headerlink" title="Object localization"></a>Object localization</h2><p>监督学习<br>训练样本中给出bounding box的中心点坐标和长宽，bx,by,bw,bh<br>Need to output bx,by,bw,bh,class label and Pc(is there any object?存在物体的置信度。当不存在物体的时候，loss函数只需计算Pc的准确度，其他项都不用考虑)。Y的维数为（1+4+class labels)。</p><hr><h2 id="Landmark-detection"><a href="#Landmark-detection" class="headerlink" title="Landmark detection"></a>Landmark detection</h2><p>特征点检测</p><p>需要输出一个是否有物体的置信度Pc,以及每个特征点的（x,y）坐标，所有训练集样本需包含这些labels。</p><p>面部表情识别，人体关键点检测。</p><hr><h2 id="Object-detection"><a href="#Object-detection" class="headerlink" title="Object detection"></a>Object detection</h2><h3 id="sliding-windows"><a href="#sliding-windows" class="headerlink" title="sliding windows"></a>sliding windows</h3><p>训练：根据滑动窗口的大小，使用适当裁剪的训练集样本，训练是否图片中有需要检测的物体物体。<br>检测：滑动窗口目标检测，对每一个窗口做一次检测，得到一个label,直到遍历图像每个区域。<br>缺点：computation expensive。步幅大时，粗粒度会影响检测性能，无法准确定位；细粒度或小步幅时计算成本高。所以一般使用简单的线性分类器。</p><h3 id="convolutional-implementation-of-sliding-windows"><a href="#convolutional-implementation-of-sliding-windows" class="headerlink" title="convolutional implementation of sliding windows"></a>convolutional implementation of sliding windows</h3><p>Turning FC layer into convolutional layers<br>没有全连接层时，输入图片的大小可以改变。<br>该卷积操作的原理是：不需要将需检测的输入图片分割成滑窗的大小的子集，分贝执行前向传播；而是直接将图片输入给卷积网络，其中的公共区域可以共享很多计算，一次性输出所有检测值。但边界框位置可能不够准确。</p><h3 id="bounding-box-prediction"><a href="#bounding-box-prediction" class="headerlink" title="bounding box prediction"></a>bounding box prediction</h3><p>YOLO algorithm</p><p>一个格子只存在一个物体时<br>用一个网格划分输入图片（3<em>3）<br>Labels for training for each grid cell:置信度Pc，bx,by,bw,bh, class labels<br>将物体分配给中心所在的格子(训练集中该格子的Pc=1)<br>output dimension:(1+4+class labels)\</em>3*3,<br>bx,by,bw,bh are specified relative to the grid cell(bbox中心所在cell的左上角坐标为0，右下角为1，bx,by是相对（0,0)的偏移量，值在0到1之间；bw,bh是bbox的长宽与grid cell边长的比值，可大于1。可以用sigmoid等函数处理，使其位于0到1之间）</p><h3 id="Intersection-over-Union"><a href="#Intersection-over-Union" class="headerlink" title="Intersection over Union"></a>Intersection over Union</h3><p>“Correct” if IoU&gt;=0.5(human chosen)<br>the predicted bbox with the ground truth(prior bbox)</p><h3 id="Non-max-Suppresion"><a href="#Non-max-Suppresion" class="headerlink" title="Non-max Suppresion"></a>Non-max Suppresion</h3><p>确保每个物体只被检测出一次。<br>先去掉所有Pc小于一定阈值的边界框(认为没有检测到物体），找出检测结果中Pc最大的bbox,然后去掉与其IoU大于阈值的检测框；再选择剩下的bbox中概率最高的…直到处理完所有框。<br>如果图像中有多中类别的物体，应该分别对每种label各<strong>独立</strong>做一次NMS。</p><hr><h3 id="Anchor-box"><a href="#Anchor-box" class="headerlink" title="Anchor box"></a>Anchor box</h3><p>使一个grid cell可以检测出多个物体。</p><p><strong>Previously</strong>:<br>each object in training image is assigned to grid cell that contains that object’s midpoint.</p><p><strong>With anchor boxes</strong>:<br>each object in training image is assigned to grid cell that contains object’s midpoint and anchor box for the grid cell with highest IoU. match with <strong>(grid cell, anchor box)</strong>.</p><p>for each grid cell, the output Y dimension is (1+4+class labels)*anchor boxes.</p><p>use k-means algorithm to choose anchor box.</p><p>grid cells越多，一个cell中出现多个物体的可能性更小。</p><hr><h2 id="Region-proposal-network-RPN"><a href="#Region-proposal-network-RPN" class="headerlink" title="Region proposal network(RPN)"></a>Region proposal network(RPN)</h2><p>two stages<br>利用图像分割算法，选出候选区域，在每种色块(blobs)上跑分类器。先找出可能的2000个色块，然后在这些色块上放置bbox，在这些色块上跑分类器。</p><p>R-CNN: Propose regions. Classify proposed regions one at a time. Output label + bounding box.</p><p>Fast R-CNN: Propose regions. Useconvolution implementation of sliding windows to classify all the proposed regions.</p><p>Faster R-CNN: Use convolutional network to propose regions.</p>]]></content:encoded>
      
      <comments>http://yoursite.com/2018/11/27/notes_for_dl/#disqus_thread</comments>
    </item>
    
  </channel>
</rss>
